{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 6.0,
  "eval_steps": 500,
  "global_step": 4140,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014498006524102935,
      "grad_norm": 52.953208923339844,
      "learning_rate": 9.987922705314011e-05,
      "loss": 81.16573486328124,
      "step": 10
    },
    {
      "epoch": 0.02899601304820587,
      "grad_norm": 75.87570190429688,
      "learning_rate": 9.96376811594203e-05,
      "loss": 75.47073974609376,
      "step": 20
    },
    {
      "epoch": 0.043494019572308806,
      "grad_norm": 70.46574401855469,
      "learning_rate": 9.939613526570049e-05,
      "loss": 67.06438598632812,
      "step": 30
    },
    {
      "epoch": 0.05799202609641174,
      "grad_norm": 99.88404846191406,
      "learning_rate": 9.917874396135266e-05,
      "loss": 58.867626953125,
      "step": 40
    },
    {
      "epoch": 0.07249003262051468,
      "grad_norm": 113.21993255615234,
      "learning_rate": 9.893719806763286e-05,
      "loss": 49.740194702148436,
      "step": 50
    },
    {
      "epoch": 0.08698803914461761,
      "grad_norm": 167.98353576660156,
      "learning_rate": 9.869565217391305e-05,
      "loss": 36.76742248535156,
      "step": 60
    },
    {
      "epoch": 0.10148604566872055,
      "grad_norm": 121.95467376708984,
      "learning_rate": 9.845410628019324e-05,
      "loss": 29.155545043945313,
      "step": 70
    },
    {
      "epoch": 0.11598405219282348,
      "grad_norm": 110.28437042236328,
      "learning_rate": 9.821256038647345e-05,
      "loss": 20.250752258300782,
      "step": 80
    },
    {
      "epoch": 0.13048205871692642,
      "grad_norm": 58.974159240722656,
      "learning_rate": 9.797101449275364e-05,
      "loss": 14.97198944091797,
      "step": 90
    },
    {
      "epoch": 0.14498006524102935,
      "grad_norm": 60.44218444824219,
      "learning_rate": 9.772946859903383e-05,
      "loss": 14.490728759765625,
      "step": 100
    },
    {
      "epoch": 0.1594780717651323,
      "grad_norm": 59.29489517211914,
      "learning_rate": 9.748792270531402e-05,
      "loss": 9.727249908447266,
      "step": 110
    },
    {
      "epoch": 0.17397607828923523,
      "grad_norm": 54.771446228027344,
      "learning_rate": 9.724637681159421e-05,
      "loss": 10.396581268310547,
      "step": 120
    },
    {
      "epoch": 0.18847408481333816,
      "grad_norm": 49.146888732910156,
      "learning_rate": 9.70048309178744e-05,
      "loss": 9.144062042236328,
      "step": 130
    },
    {
      "epoch": 0.2029720913374411,
      "grad_norm": 35.8249626159668,
      "learning_rate": 9.67632850241546e-05,
      "loss": 8.774761962890626,
      "step": 140
    },
    {
      "epoch": 0.21747009786154403,
      "grad_norm": 55.290096282958984,
      "learning_rate": 9.652173913043479e-05,
      "loss": 8.989910125732422,
      "step": 150
    },
    {
      "epoch": 0.23196810438564697,
      "grad_norm": 43.119327545166016,
      "learning_rate": 9.628019323671498e-05,
      "loss": 8.420367431640624,
      "step": 160
    },
    {
      "epoch": 0.2464661109097499,
      "grad_norm": 70.40753936767578,
      "learning_rate": 9.603864734299517e-05,
      "loss": 6.049578094482422,
      "step": 170
    },
    {
      "epoch": 0.26096411743385284,
      "grad_norm": 70.84967041015625,
      "learning_rate": 9.579710144927536e-05,
      "loss": 8.27963638305664,
      "step": 180
    },
    {
      "epoch": 0.2754621239579558,
      "grad_norm": 36.7550163269043,
      "learning_rate": 9.555555555555557e-05,
      "loss": 7.730546569824218,
      "step": 190
    },
    {
      "epoch": 0.2899601304820587,
      "grad_norm": 51.34489059448242,
      "learning_rate": 9.531400966183576e-05,
      "loss": 6.559477996826172,
      "step": 200
    },
    {
      "epoch": 0.30445813700616164,
      "grad_norm": 60.85143280029297,
      "learning_rate": 9.507246376811595e-05,
      "loss": 6.093899154663086,
      "step": 210
    },
    {
      "epoch": 0.3189561435302646,
      "grad_norm": 50.43595504760742,
      "learning_rate": 9.483091787439614e-05,
      "loss": 8.845671081542969,
      "step": 220
    },
    {
      "epoch": 0.3334541500543675,
      "grad_norm": 47.95978546142578,
      "learning_rate": 9.458937198067634e-05,
      "loss": 7.263516235351562,
      "step": 230
    },
    {
      "epoch": 0.34795215657847045,
      "grad_norm": 42.94090270996094,
      "learning_rate": 9.434782608695653e-05,
      "loss": 7.319622039794922,
      "step": 240
    },
    {
      "epoch": 0.3624501631025734,
      "grad_norm": 55.40592956542969,
      "learning_rate": 9.410628019323672e-05,
      "loss": 6.690677642822266,
      "step": 250
    },
    {
      "epoch": 0.3769481696266763,
      "grad_norm": 52.548336029052734,
      "learning_rate": 9.386473429951691e-05,
      "loss": 6.103939056396484,
      "step": 260
    },
    {
      "epoch": 0.39144617615077926,
      "grad_norm": 77.33299255371094,
      "learning_rate": 9.36231884057971e-05,
      "loss": 6.880562591552734,
      "step": 270
    },
    {
      "epoch": 0.4059441826748822,
      "grad_norm": 51.52708435058594,
      "learning_rate": 9.33816425120773e-05,
      "loss": 7.398991394042969,
      "step": 280
    },
    {
      "epoch": 0.42044218919898513,
      "grad_norm": 30.936405181884766,
      "learning_rate": 9.31400966183575e-05,
      "loss": 7.560799407958984,
      "step": 290
    },
    {
      "epoch": 0.43494019572308806,
      "grad_norm": 74.67315673828125,
      "learning_rate": 9.289855072463769e-05,
      "loss": 6.427009582519531,
      "step": 300
    },
    {
      "epoch": 0.449438202247191,
      "grad_norm": 30.441120147705078,
      "learning_rate": 9.265700483091788e-05,
      "loss": 6.3301342010498045,
      "step": 310
    },
    {
      "epoch": 0.46393620877129393,
      "grad_norm": 46.841583251953125,
      "learning_rate": 9.241545893719807e-05,
      "loss": 5.225358581542968,
      "step": 320
    },
    {
      "epoch": 0.47843421529539687,
      "grad_norm": 47.1569709777832,
      "learning_rate": 9.217391304347827e-05,
      "loss": 6.4608612060546875,
      "step": 330
    },
    {
      "epoch": 0.4929322218194998,
      "grad_norm": 50.973297119140625,
      "learning_rate": 9.193236714975846e-05,
      "loss": 6.753641510009766,
      "step": 340
    },
    {
      "epoch": 0.5074302283436027,
      "grad_norm": 31.932647705078125,
      "learning_rate": 9.169082125603865e-05,
      "loss": 6.369433212280273,
      "step": 350
    },
    {
      "epoch": 0.5219282348677057,
      "grad_norm": 46.18975067138672,
      "learning_rate": 9.144927536231884e-05,
      "loss": 6.658008575439453,
      "step": 360
    },
    {
      "epoch": 0.5364262413918086,
      "grad_norm": 44.30972671508789,
      "learning_rate": 9.120772946859903e-05,
      "loss": 5.135572052001953,
      "step": 370
    },
    {
      "epoch": 0.5509242479159115,
      "grad_norm": 50.241554260253906,
      "learning_rate": 9.096618357487923e-05,
      "loss": 6.673595428466797,
      "step": 380
    },
    {
      "epoch": 0.5654222544400145,
      "grad_norm": 40.09040832519531,
      "learning_rate": 9.072463768115943e-05,
      "loss": 5.930867767333984,
      "step": 390
    },
    {
      "epoch": 0.5799202609641174,
      "grad_norm": 61.40766906738281,
      "learning_rate": 9.048309178743962e-05,
      "loss": 6.163790130615235,
      "step": 400
    },
    {
      "epoch": 0.5944182674882204,
      "grad_norm": 84.41061401367188,
      "learning_rate": 9.024154589371981e-05,
      "loss": 5.660062408447265,
      "step": 410
    },
    {
      "epoch": 0.6089162740123233,
      "grad_norm": 37.31785202026367,
      "learning_rate": 9e-05,
      "loss": 6.56937255859375,
      "step": 420
    },
    {
      "epoch": 0.6234142805364262,
      "grad_norm": 64.94850158691406,
      "learning_rate": 8.97584541062802e-05,
      "loss": 6.303755187988282,
      "step": 430
    },
    {
      "epoch": 0.6379122870605292,
      "grad_norm": 39.29319763183594,
      "learning_rate": 8.951690821256039e-05,
      "loss": 4.59997444152832,
      "step": 440
    },
    {
      "epoch": 0.6524102935846321,
      "grad_norm": 69.27129364013672,
      "learning_rate": 8.927536231884058e-05,
      "loss": 6.890204620361328,
      "step": 450
    },
    {
      "epoch": 0.666908300108735,
      "grad_norm": 57.79429244995117,
      "learning_rate": 8.903381642512077e-05,
      "loss": 5.497145080566407,
      "step": 460
    },
    {
      "epoch": 0.681406306632838,
      "grad_norm": 56.49931716918945,
      "learning_rate": 8.879227053140096e-05,
      "loss": 5.569547271728515,
      "step": 470
    },
    {
      "epoch": 0.6959043131569409,
      "grad_norm": 37.23007583618164,
      "learning_rate": 8.855072463768116e-05,
      "loss": 4.851523971557617,
      "step": 480
    },
    {
      "epoch": 0.7104023196810438,
      "grad_norm": 42.865596771240234,
      "learning_rate": 8.830917874396135e-05,
      "loss": 6.756415557861328,
      "step": 490
    },
    {
      "epoch": 0.7249003262051468,
      "grad_norm": 47.6230583190918,
      "learning_rate": 8.806763285024155e-05,
      "loss": 4.534558486938477,
      "step": 500
    },
    {
      "epoch": 0.7393983327292497,
      "grad_norm": 44.51069259643555,
      "learning_rate": 8.782608695652174e-05,
      "loss": 6.134165191650391,
      "step": 510
    },
    {
      "epoch": 0.7538963392533526,
      "grad_norm": 40.9496955871582,
      "learning_rate": 8.758454106280194e-05,
      "loss": 6.644511413574219,
      "step": 520
    },
    {
      "epoch": 0.7683943457774556,
      "grad_norm": 53.77290725708008,
      "learning_rate": 8.734299516908213e-05,
      "loss": 5.295065307617188,
      "step": 530
    },
    {
      "epoch": 0.7828923523015585,
      "grad_norm": 66.18285369873047,
      "learning_rate": 8.710144927536232e-05,
      "loss": 5.573646926879883,
      "step": 540
    },
    {
      "epoch": 0.7973903588256614,
      "grad_norm": 64.8251953125,
      "learning_rate": 8.685990338164251e-05,
      "loss": 5.177457809448242,
      "step": 550
    },
    {
      "epoch": 0.8118883653497644,
      "grad_norm": 57.60697937011719,
      "learning_rate": 8.66183574879227e-05,
      "loss": 7.968142700195313,
      "step": 560
    },
    {
      "epoch": 0.8263863718738673,
      "grad_norm": 52.402931213378906,
      "learning_rate": 8.63768115942029e-05,
      "loss": 6.0530517578125,
      "step": 570
    },
    {
      "epoch": 0.8408843783979703,
      "grad_norm": 44.61121368408203,
      "learning_rate": 8.613526570048309e-05,
      "loss": 5.752162170410156,
      "step": 580
    },
    {
      "epoch": 0.8553823849220732,
      "grad_norm": 21.98896598815918,
      "learning_rate": 8.589371980676328e-05,
      "loss": 5.824652099609375,
      "step": 590
    },
    {
      "epoch": 0.8698803914461761,
      "grad_norm": 32.644256591796875,
      "learning_rate": 8.565217391304348e-05,
      "loss": 5.0410713195800785,
      "step": 600
    },
    {
      "epoch": 0.8843783979702791,
      "grad_norm": 67.93643188476562,
      "learning_rate": 8.541062801932368e-05,
      "loss": 5.540287780761719,
      "step": 610
    },
    {
      "epoch": 0.898876404494382,
      "grad_norm": 29.484020233154297,
      "learning_rate": 8.516908212560387e-05,
      "loss": 6.235525512695313,
      "step": 620
    },
    {
      "epoch": 0.9133744110184849,
      "grad_norm": 52.53120803833008,
      "learning_rate": 8.492753623188406e-05,
      "loss": 6.259186553955078,
      "step": 630
    },
    {
      "epoch": 0.9278724175425879,
      "grad_norm": 51.75511932373047,
      "learning_rate": 8.468599033816425e-05,
      "loss": 6.3732952117919925,
      "step": 640
    },
    {
      "epoch": 0.9423704240666908,
      "grad_norm": 51.73725509643555,
      "learning_rate": 8.444444444444444e-05,
      "loss": 5.25970458984375,
      "step": 650
    },
    {
      "epoch": 0.9568684305907937,
      "grad_norm": 52.771636962890625,
      "learning_rate": 8.420289855072463e-05,
      "loss": 5.776855087280273,
      "step": 660
    },
    {
      "epoch": 0.9713664371148967,
      "grad_norm": 36.076866149902344,
      "learning_rate": 8.396135265700483e-05,
      "loss": 5.131585693359375,
      "step": 670
    },
    {
      "epoch": 0.9858644436389996,
      "grad_norm": 40.04486846923828,
      "learning_rate": 8.371980676328502e-05,
      "loss": 4.821474075317383,
      "step": 680
    },
    {
      "epoch": 1.0,
      "grad_norm": 50.043190002441406,
      "learning_rate": 8.347826086956521e-05,
      "loss": 4.34370231628418,
      "step": 690
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.6724161505699158,
      "eval_runtime": 13.6339,
      "eval_samples_per_second": 4.107,
      "eval_steps_per_second": 0.513,
      "step": 690
    },
    {
      "epoch": 1.014498006524103,
      "grad_norm": 28.811506271362305,
      "learning_rate": 8.323671497584542e-05,
      "loss": 4.284936141967774,
      "step": 700
    },
    {
      "epoch": 1.0289960130482059,
      "grad_norm": 53.82708740234375,
      "learning_rate": 8.299516908212561e-05,
      "loss": 3.989913558959961,
      "step": 710
    },
    {
      "epoch": 1.0434940195723088,
      "grad_norm": 38.94517517089844,
      "learning_rate": 8.27536231884058e-05,
      "loss": 5.463456344604492,
      "step": 720
    },
    {
      "epoch": 1.0579920260964117,
      "grad_norm": 37.178428649902344,
      "learning_rate": 8.251207729468599e-05,
      "loss": 4.689655303955078,
      "step": 730
    },
    {
      "epoch": 1.0724900326205147,
      "grad_norm": 54.137393951416016,
      "learning_rate": 8.227053140096618e-05,
      "loss": 4.65234603881836,
      "step": 740
    },
    {
      "epoch": 1.0869880391446176,
      "grad_norm": 44.938758850097656,
      "learning_rate": 8.202898550724637e-05,
      "loss": 4.118959426879883,
      "step": 750
    },
    {
      "epoch": 1.1014860456687205,
      "grad_norm": 35.1382942199707,
      "learning_rate": 8.178743961352658e-05,
      "loss": 4.780020523071289,
      "step": 760
    },
    {
      "epoch": 1.1159840521928235,
      "grad_norm": 29.09222412109375,
      "learning_rate": 8.154589371980677e-05,
      "loss": 4.454275512695313,
      "step": 770
    },
    {
      "epoch": 1.1304820587169264,
      "grad_norm": 51.51734161376953,
      "learning_rate": 8.130434782608696e-05,
      "loss": 4.404365539550781,
      "step": 780
    },
    {
      "epoch": 1.1449800652410294,
      "grad_norm": 37.45249557495117,
      "learning_rate": 8.106280193236715e-05,
      "loss": 5.327708053588867,
      "step": 790
    },
    {
      "epoch": 1.1594780717651323,
      "grad_norm": 37.677703857421875,
      "learning_rate": 8.084541062801933e-05,
      "loss": 4.833238983154297,
      "step": 800
    },
    {
      "epoch": 1.1739760782892352,
      "grad_norm": 48.23582077026367,
      "learning_rate": 8.060386473429952e-05,
      "loss": 4.903010559082031,
      "step": 810
    },
    {
      "epoch": 1.1884740848133382,
      "grad_norm": 38.820465087890625,
      "learning_rate": 8.036231884057971e-05,
      "loss": 4.4055328369140625,
      "step": 820
    },
    {
      "epoch": 1.202972091337441,
      "grad_norm": 47.7231330871582,
      "learning_rate": 8.012077294685992e-05,
      "loss": 4.871735382080078,
      "step": 830
    },
    {
      "epoch": 1.217470097861544,
      "grad_norm": 25.357194900512695,
      "learning_rate": 7.987922705314011e-05,
      "loss": 4.3767955780029295,
      "step": 840
    },
    {
      "epoch": 1.231968104385647,
      "grad_norm": 47.53919219970703,
      "learning_rate": 7.96376811594203e-05,
      "loss": 4.036569213867187,
      "step": 850
    },
    {
      "epoch": 1.24646611090975,
      "grad_norm": 32.88642883300781,
      "learning_rate": 7.939613526570049e-05,
      "loss": 4.321671676635742,
      "step": 860
    },
    {
      "epoch": 1.2609641174338528,
      "grad_norm": 38.995235443115234,
      "learning_rate": 7.915458937198068e-05,
      "loss": 4.476469039916992,
      "step": 870
    },
    {
      "epoch": 1.2754621239579558,
      "grad_norm": 50.485225677490234,
      "learning_rate": 7.891304347826088e-05,
      "loss": 4.509273529052734,
      "step": 880
    },
    {
      "epoch": 1.2899601304820587,
      "grad_norm": 48.31428909301758,
      "learning_rate": 7.867149758454107e-05,
      "loss": 5.658724212646485,
      "step": 890
    },
    {
      "epoch": 1.3044581370061616,
      "grad_norm": 44.77791976928711,
      "learning_rate": 7.842995169082126e-05,
      "loss": 5.077752304077149,
      "step": 900
    },
    {
      "epoch": 1.3189561435302646,
      "grad_norm": 64.38499450683594,
      "learning_rate": 7.818840579710145e-05,
      "loss": 4.195296859741211,
      "step": 910
    },
    {
      "epoch": 1.3334541500543675,
      "grad_norm": 45.17753601074219,
      "learning_rate": 7.794685990338164e-05,
      "loss": 4.527527618408203,
      "step": 920
    },
    {
      "epoch": 1.3479521565784705,
      "grad_norm": 80.29609680175781,
      "learning_rate": 7.770531400966185e-05,
      "loss": 6.464712524414063,
      "step": 930
    },
    {
      "epoch": 1.3624501631025734,
      "grad_norm": 56.696563720703125,
      "learning_rate": 7.746376811594204e-05,
      "loss": 5.963042449951172,
      "step": 940
    },
    {
      "epoch": 1.3769481696266763,
      "grad_norm": 55.07769775390625,
      "learning_rate": 7.722222222222223e-05,
      "loss": 4.18743667602539,
      "step": 950
    },
    {
      "epoch": 1.3914461761507793,
      "grad_norm": 66.86894226074219,
      "learning_rate": 7.698067632850242e-05,
      "loss": 5.186980438232422,
      "step": 960
    },
    {
      "epoch": 1.4059441826748822,
      "grad_norm": 45.04023361206055,
      "learning_rate": 7.673913043478262e-05,
      "loss": 4.14722785949707,
      "step": 970
    },
    {
      "epoch": 1.4204421891989851,
      "grad_norm": 50.083560943603516,
      "learning_rate": 7.649758454106281e-05,
      "loss": 4.477280807495117,
      "step": 980
    },
    {
      "epoch": 1.434940195723088,
      "grad_norm": 41.58301544189453,
      "learning_rate": 7.6256038647343e-05,
      "loss": 4.781556701660156,
      "step": 990
    },
    {
      "epoch": 1.449438202247191,
      "grad_norm": 37.609100341796875,
      "learning_rate": 7.601449275362319e-05,
      "loss": 4.522645568847656,
      "step": 1000
    },
    {
      "epoch": 1.463936208771294,
      "grad_norm": 27.400983810424805,
      "learning_rate": 7.577294685990338e-05,
      "loss": 3.5927764892578127,
      "step": 1010
    },
    {
      "epoch": 1.4784342152953969,
      "grad_norm": 42.04196548461914,
      "learning_rate": 7.553140096618357e-05,
      "loss": 5.120228576660156,
      "step": 1020
    },
    {
      "epoch": 1.4929322218194998,
      "grad_norm": 28.9537353515625,
      "learning_rate": 7.528985507246377e-05,
      "loss": 4.054245376586914,
      "step": 1030
    },
    {
      "epoch": 1.5074302283436027,
      "grad_norm": 49.471988677978516,
      "learning_rate": 7.504830917874397e-05,
      "loss": 4.207571411132813,
      "step": 1040
    },
    {
      "epoch": 1.5219282348677057,
      "grad_norm": 51.77118682861328,
      "learning_rate": 7.480676328502416e-05,
      "loss": 4.7833820343017575,
      "step": 1050
    },
    {
      "epoch": 1.5364262413918086,
      "grad_norm": 43.03315353393555,
      "learning_rate": 7.456521739130435e-05,
      "loss": 4.574877548217773,
      "step": 1060
    },
    {
      "epoch": 1.5509242479159115,
      "grad_norm": 48.36722183227539,
      "learning_rate": 7.432367149758455e-05,
      "loss": 5.454948806762696,
      "step": 1070
    },
    {
      "epoch": 1.5654222544400145,
      "grad_norm": 41.66373062133789,
      "learning_rate": 7.408212560386474e-05,
      "loss": 5.845404815673828,
      "step": 1080
    },
    {
      "epoch": 1.5799202609641174,
      "grad_norm": 47.87042999267578,
      "learning_rate": 7.384057971014493e-05,
      "loss": 4.927184677124023,
      "step": 1090
    },
    {
      "epoch": 1.5944182674882204,
      "grad_norm": 58.235477447509766,
      "learning_rate": 7.359903381642512e-05,
      "loss": 4.971090316772461,
      "step": 1100
    },
    {
      "epoch": 1.6089162740123233,
      "grad_norm": 82.79959869384766,
      "learning_rate": 7.335748792270531e-05,
      "loss": 5.012945175170898,
      "step": 1110
    },
    {
      "epoch": 1.6234142805364262,
      "grad_norm": 53.97639846801758,
      "learning_rate": 7.31159420289855e-05,
      "loss": 4.769107437133789,
      "step": 1120
    },
    {
      "epoch": 1.6379122870605292,
      "grad_norm": 47.19252395629883,
      "learning_rate": 7.28743961352657e-05,
      "loss": 4.689352798461914,
      "step": 1130
    },
    {
      "epoch": 1.652410293584632,
      "grad_norm": 53.79071807861328,
      "learning_rate": 7.26328502415459e-05,
      "loss": 4.078671264648437,
      "step": 1140
    },
    {
      "epoch": 1.666908300108735,
      "grad_norm": 33.07976150512695,
      "learning_rate": 7.23913043478261e-05,
      "loss": 4.46434440612793,
      "step": 1150
    },
    {
      "epoch": 1.681406306632838,
      "grad_norm": 47.126224517822266,
      "learning_rate": 7.214975845410629e-05,
      "loss": 3.3983989715576173,
      "step": 1160
    },
    {
      "epoch": 1.695904313156941,
      "grad_norm": 30.298751831054688,
      "learning_rate": 7.190821256038648e-05,
      "loss": 5.366864013671875,
      "step": 1170
    },
    {
      "epoch": 1.7104023196810438,
      "grad_norm": 36.489044189453125,
      "learning_rate": 7.166666666666667e-05,
      "loss": 4.54273796081543,
      "step": 1180
    },
    {
      "epoch": 1.7249003262051468,
      "grad_norm": 43.169246673583984,
      "learning_rate": 7.142512077294686e-05,
      "loss": 4.703837585449219,
      "step": 1190
    },
    {
      "epoch": 1.7393983327292497,
      "grad_norm": 67.99071502685547,
      "learning_rate": 7.118357487922705e-05,
      "loss": 4.331568908691406,
      "step": 1200
    },
    {
      "epoch": 1.7538963392533526,
      "grad_norm": 49.43693542480469,
      "learning_rate": 7.094202898550724e-05,
      "loss": 4.505270004272461,
      "step": 1210
    },
    {
      "epoch": 1.7683943457774556,
      "grad_norm": 61.655765533447266,
      "learning_rate": 7.070048309178744e-05,
      "loss": 4.362937545776367,
      "step": 1220
    },
    {
      "epoch": 1.7828923523015585,
      "grad_norm": 46.86510467529297,
      "learning_rate": 7.045893719806763e-05,
      "loss": 4.216183853149414,
      "step": 1230
    },
    {
      "epoch": 1.7973903588256614,
      "grad_norm": 54.17262649536133,
      "learning_rate": 7.021739130434783e-05,
      "loss": 4.776254272460937,
      "step": 1240
    },
    {
      "epoch": 1.8118883653497644,
      "grad_norm": 60.813358306884766,
      "learning_rate": 6.997584541062803e-05,
      "loss": 5.090281295776367,
      "step": 1250
    },
    {
      "epoch": 1.8263863718738673,
      "grad_norm": 67.51483154296875,
      "learning_rate": 6.973429951690822e-05,
      "loss": 4.62181510925293,
      "step": 1260
    },
    {
      "epoch": 1.8408843783979703,
      "grad_norm": 38.856895446777344,
      "learning_rate": 6.949275362318841e-05,
      "loss": 4.616135025024414,
      "step": 1270
    },
    {
      "epoch": 1.8553823849220732,
      "grad_norm": 29.445152282714844,
      "learning_rate": 6.92512077294686e-05,
      "loss": 4.003216552734375,
      "step": 1280
    },
    {
      "epoch": 1.8698803914461761,
      "grad_norm": 50.002113342285156,
      "learning_rate": 6.900966183574879e-05,
      "loss": 3.9539535522460936,
      "step": 1290
    },
    {
      "epoch": 1.884378397970279,
      "grad_norm": 39.33277893066406,
      "learning_rate": 6.876811594202898e-05,
      "loss": 3.4740795135498046,
      "step": 1300
    },
    {
      "epoch": 1.898876404494382,
      "grad_norm": 36.550785064697266,
      "learning_rate": 6.852657004830918e-05,
      "loss": 4.2887123107910154,
      "step": 1310
    },
    {
      "epoch": 1.913374411018485,
      "grad_norm": 47.07185363769531,
      "learning_rate": 6.828502415458937e-05,
      "loss": 3.725658416748047,
      "step": 1320
    },
    {
      "epoch": 1.9278724175425879,
      "grad_norm": 54.06242370605469,
      "learning_rate": 6.804347826086956e-05,
      "loss": 4.294247436523437,
      "step": 1330
    },
    {
      "epoch": 1.9423704240666908,
      "grad_norm": 45.642662048339844,
      "learning_rate": 6.780193236714975e-05,
      "loss": 4.043934631347656,
      "step": 1340
    },
    {
      "epoch": 1.9568684305907937,
      "grad_norm": 66.90531921386719,
      "learning_rate": 6.756038647342996e-05,
      "loss": 5.090407943725586,
      "step": 1350
    },
    {
      "epoch": 1.9713664371148967,
      "grad_norm": 46.25623321533203,
      "learning_rate": 6.731884057971015e-05,
      "loss": 4.260204696655274,
      "step": 1360
    },
    {
      "epoch": 1.9858644436389996,
      "grad_norm": 59.67180633544922,
      "learning_rate": 6.707729468599034e-05,
      "loss": 3.908051300048828,
      "step": 1370
    },
    {
      "epoch": 2.0,
      "grad_norm": 44.977169036865234,
      "learning_rate": 6.683574879227053e-05,
      "loss": 4.352987670898438,
      "step": 1380
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.531622588634491,
      "eval_runtime": 5.925,
      "eval_samples_per_second": 9.452,
      "eval_steps_per_second": 1.181,
      "step": 1380
    },
    {
      "epoch": 2.0144980065241027,
      "grad_norm": 39.353763580322266,
      "learning_rate": 6.659420289855072e-05,
      "loss": 4.225373077392578,
      "step": 1390
    },
    {
      "epoch": 2.028996013048206,
      "grad_norm": 71.43448638916016,
      "learning_rate": 6.635265700483092e-05,
      "loss": 3.311750793457031,
      "step": 1400
    },
    {
      "epoch": 2.043494019572309,
      "grad_norm": 43.31686782836914,
      "learning_rate": 6.611111111111111e-05,
      "loss": 3.410274887084961,
      "step": 1410
    },
    {
      "epoch": 2.0579920260964117,
      "grad_norm": 46.06797409057617,
      "learning_rate": 6.58695652173913e-05,
      "loss": 4.1458080291748045,
      "step": 1420
    },
    {
      "epoch": 2.0724900326205145,
      "grad_norm": 34.07244873046875,
      "learning_rate": 6.562801932367149e-05,
      "loss": 4.115389251708985,
      "step": 1430
    },
    {
      "epoch": 2.0869880391446176,
      "grad_norm": 48.63488006591797,
      "learning_rate": 6.53864734299517e-05,
      "loss": 3.709790802001953,
      "step": 1440
    },
    {
      "epoch": 2.1014860456687208,
      "grad_norm": 42.837806701660156,
      "learning_rate": 6.514492753623189e-05,
      "loss": 4.62970085144043,
      "step": 1450
    },
    {
      "epoch": 2.1159840521928235,
      "grad_norm": 53.02653503417969,
      "learning_rate": 6.490338164251208e-05,
      "loss": 3.8068904876708984,
      "step": 1460
    },
    {
      "epoch": 2.130482058716926,
      "grad_norm": 62.57509231567383,
      "learning_rate": 6.466183574879227e-05,
      "loss": 4.07603874206543,
      "step": 1470
    },
    {
      "epoch": 2.1449800652410294,
      "grad_norm": 53.51307678222656,
      "learning_rate": 6.442028985507246e-05,
      "loss": 3.592708969116211,
      "step": 1480
    },
    {
      "epoch": 2.1594780717651325,
      "grad_norm": 35.77006530761719,
      "learning_rate": 6.417874396135265e-05,
      "loss": 3.3929061889648438,
      "step": 1490
    },
    {
      "epoch": 2.1739760782892352,
      "grad_norm": 37.39327621459961,
      "learning_rate": 6.393719806763286e-05,
      "loss": 4.485673522949218,
      "step": 1500
    },
    {
      "epoch": 2.188474084813338,
      "grad_norm": 36.317195892333984,
      "learning_rate": 6.369565217391305e-05,
      "loss": 4.282230377197266,
      "step": 1510
    },
    {
      "epoch": 2.202972091337441,
      "grad_norm": 35.65131378173828,
      "learning_rate": 6.345410628019324e-05,
      "loss": 3.5853500366210938,
      "step": 1520
    },
    {
      "epoch": 2.2174700978615443,
      "grad_norm": 46.71910095214844,
      "learning_rate": 6.321256038647343e-05,
      "loss": 2.6873601913452148,
      "step": 1530
    },
    {
      "epoch": 2.231968104385647,
      "grad_norm": 41.91736602783203,
      "learning_rate": 6.297101449275363e-05,
      "loss": 3.3779884338378907,
      "step": 1540
    },
    {
      "epoch": 2.2464661109097497,
      "grad_norm": 52.140106201171875,
      "learning_rate": 6.272946859903382e-05,
      "loss": 4.007538223266602,
      "step": 1550
    },
    {
      "epoch": 2.260964117433853,
      "grad_norm": 39.09035110473633,
      "learning_rate": 6.248792270531402e-05,
      "loss": 4.705059814453125,
      "step": 1560
    },
    {
      "epoch": 2.275462123957956,
      "grad_norm": 48.371124267578125,
      "learning_rate": 6.224637681159422e-05,
      "loss": 3.9435565948486326,
      "step": 1570
    },
    {
      "epoch": 2.2899601304820587,
      "grad_norm": 53.51891326904297,
      "learning_rate": 6.200483091787441e-05,
      "loss": 4.172101211547852,
      "step": 1580
    },
    {
      "epoch": 2.3044581370061614,
      "grad_norm": 53.0815315246582,
      "learning_rate": 6.17632850241546e-05,
      "loss": 4.693632125854492,
      "step": 1590
    },
    {
      "epoch": 2.3189561435302646,
      "grad_norm": 57.42876052856445,
      "learning_rate": 6.152173913043479e-05,
      "loss": 4.354703521728515,
      "step": 1600
    },
    {
      "epoch": 2.3334541500543677,
      "grad_norm": 70.0052261352539,
      "learning_rate": 6.128019323671498e-05,
      "loss": 4.52601203918457,
      "step": 1610
    },
    {
      "epoch": 2.3479521565784705,
      "grad_norm": 28.337514877319336,
      "learning_rate": 6.103864734299517e-05,
      "loss": 3.6512161254882813,
      "step": 1620
    },
    {
      "epoch": 2.362450163102573,
      "grad_norm": 52.084712982177734,
      "learning_rate": 6.079710144927536e-05,
      "loss": 4.336588287353516,
      "step": 1630
    },
    {
      "epoch": 2.3769481696266763,
      "grad_norm": 33.954463958740234,
      "learning_rate": 6.055555555555555e-05,
      "loss": 3.4378665924072265,
      "step": 1640
    },
    {
      "epoch": 2.3914461761507795,
      "grad_norm": 43.140098571777344,
      "learning_rate": 6.031400966183575e-05,
      "loss": 3.7484325408935546,
      "step": 1650
    },
    {
      "epoch": 2.405944182674882,
      "grad_norm": 34.23225784301758,
      "learning_rate": 6.007246376811595e-05,
      "loss": 4.11577033996582,
      "step": 1660
    },
    {
      "epoch": 2.420442189198985,
      "grad_norm": 52.964134216308594,
      "learning_rate": 5.983091787439614e-05,
      "loss": 4.330959701538086,
      "step": 1670
    },
    {
      "epoch": 2.434940195723088,
      "grad_norm": 54.0946044921875,
      "learning_rate": 5.958937198067633e-05,
      "loss": 3.4437732696533203,
      "step": 1680
    },
    {
      "epoch": 2.449438202247191,
      "grad_norm": 52.6036491394043,
      "learning_rate": 5.934782608695652e-05,
      "loss": 4.3341724395751955,
      "step": 1690
    },
    {
      "epoch": 2.463936208771294,
      "grad_norm": 39.58100891113281,
      "learning_rate": 5.9106280193236715e-05,
      "loss": 4.084330368041992,
      "step": 1700
    },
    {
      "epoch": 2.4784342152953966,
      "grad_norm": 43.637420654296875,
      "learning_rate": 5.8864734299516914e-05,
      "loss": 3.3666015625,
      "step": 1710
    },
    {
      "epoch": 2.4929322218195,
      "grad_norm": 34.65306854248047,
      "learning_rate": 5.8623188405797105e-05,
      "loss": 3.9947437286376952,
      "step": 1720
    },
    {
      "epoch": 2.507430228343603,
      "grad_norm": 34.62343978881836,
      "learning_rate": 5.83816425120773e-05,
      "loss": 3.8459499359130858,
      "step": 1730
    },
    {
      "epoch": 2.5219282348677057,
      "grad_norm": 34.711639404296875,
      "learning_rate": 5.814009661835749e-05,
      "loss": 3.528665542602539,
      "step": 1740
    },
    {
      "epoch": 2.5364262413918084,
      "grad_norm": 53.33505630493164,
      "learning_rate": 5.789855072463768e-05,
      "loss": 3.3958160400390627,
      "step": 1750
    },
    {
      "epoch": 2.5509242479159115,
      "grad_norm": 71.68185424804688,
      "learning_rate": 5.765700483091788e-05,
      "loss": 3.92991943359375,
      "step": 1760
    },
    {
      "epoch": 2.5654222544400147,
      "grad_norm": 46.223873138427734,
      "learning_rate": 5.741545893719808e-05,
      "loss": 3.4946826934814452,
      "step": 1770
    },
    {
      "epoch": 2.5799202609641174,
      "grad_norm": 40.67702102661133,
      "learning_rate": 5.717391304347827e-05,
      "loss": 3.122196006774902,
      "step": 1780
    },
    {
      "epoch": 2.59441826748822,
      "grad_norm": 34.695682525634766,
      "learning_rate": 5.693236714975846e-05,
      "loss": 3.7038578033447265,
      "step": 1790
    },
    {
      "epoch": 2.6089162740123233,
      "grad_norm": 50.3942756652832,
      "learning_rate": 5.669082125603865e-05,
      "loss": 4.0438800811767575,
      "step": 1800
    },
    {
      "epoch": 2.6234142805364264,
      "grad_norm": 28.647680282592773,
      "learning_rate": 5.6449275362318845e-05,
      "loss": 4.276519393920898,
      "step": 1810
    },
    {
      "epoch": 2.637912287060529,
      "grad_norm": 42.12788009643555,
      "learning_rate": 5.6207729468599036e-05,
      "loss": 3.4856342315673827,
      "step": 1820
    },
    {
      "epoch": 2.652410293584632,
      "grad_norm": 39.967132568359375,
      "learning_rate": 5.596618357487923e-05,
      "loss": 4.055720520019531,
      "step": 1830
    },
    {
      "epoch": 2.666908300108735,
      "grad_norm": 66.8743667602539,
      "learning_rate": 5.572463768115942e-05,
      "loss": 3.6992332458496096,
      "step": 1840
    },
    {
      "epoch": 2.681406306632838,
      "grad_norm": 74.93436431884766,
      "learning_rate": 5.548309178743961e-05,
      "loss": 3.782866287231445,
      "step": 1850
    },
    {
      "epoch": 2.695904313156941,
      "grad_norm": 58.39387512207031,
      "learning_rate": 5.524154589371982e-05,
      "loss": 3.779807281494141,
      "step": 1860
    },
    {
      "epoch": 2.7104023196810436,
      "grad_norm": 40.080875396728516,
      "learning_rate": 5.500000000000001e-05,
      "loss": 3.439345932006836,
      "step": 1870
    },
    {
      "epoch": 2.7249003262051468,
      "grad_norm": 73.93903350830078,
      "learning_rate": 5.47584541062802e-05,
      "loss": 4.1168663024902346,
      "step": 1880
    },
    {
      "epoch": 2.73939833272925,
      "grad_norm": 65.31559753417969,
      "learning_rate": 5.451690821256039e-05,
      "loss": 4.7727611541748045,
      "step": 1890
    },
    {
      "epoch": 2.7538963392533526,
      "grad_norm": 63.081199645996094,
      "learning_rate": 5.4275362318840584e-05,
      "loss": 3.8480453491210938,
      "step": 1900
    },
    {
      "epoch": 2.7683943457774554,
      "grad_norm": 45.50204849243164,
      "learning_rate": 5.4033816425120776e-05,
      "loss": 3.6136638641357424,
      "step": 1910
    },
    {
      "epoch": 2.7828923523015585,
      "grad_norm": 36.40477752685547,
      "learning_rate": 5.379227053140097e-05,
      "loss": 3.4568862915039062,
      "step": 1920
    },
    {
      "epoch": 2.7973903588256617,
      "grad_norm": 38.02307891845703,
      "learning_rate": 5.355072463768116e-05,
      "loss": 2.7506134033203127,
      "step": 1930
    },
    {
      "epoch": 2.8118883653497644,
      "grad_norm": 48.751373291015625,
      "learning_rate": 5.330917874396135e-05,
      "loss": 3.7620105743408203,
      "step": 1940
    },
    {
      "epoch": 2.826386371873867,
      "grad_norm": 64.0514144897461,
      "learning_rate": 5.306763285024154e-05,
      "loss": 3.6212627410888674,
      "step": 1950
    },
    {
      "epoch": 2.8408843783979703,
      "grad_norm": 15.253556251525879,
      "learning_rate": 5.2826086956521735e-05,
      "loss": 3.542580795288086,
      "step": 1960
    },
    {
      "epoch": 2.8553823849220734,
      "grad_norm": 42.958404541015625,
      "learning_rate": 5.258454106280194e-05,
      "loss": 3.9110828399658204,
      "step": 1970
    },
    {
      "epoch": 2.869880391446176,
      "grad_norm": 73.57350158691406,
      "learning_rate": 5.234299516908213e-05,
      "loss": 3.925273132324219,
      "step": 1980
    },
    {
      "epoch": 2.884378397970279,
      "grad_norm": 58.67112350463867,
      "learning_rate": 5.210144927536232e-05,
      "loss": 3.550894546508789,
      "step": 1990
    },
    {
      "epoch": 2.898876404494382,
      "grad_norm": 47.01784133911133,
      "learning_rate": 5.1859903381642515e-05,
      "loss": 3.517095184326172,
      "step": 2000
    },
    {
      "epoch": 2.913374411018485,
      "grad_norm": 27.41261100769043,
      "learning_rate": 5.161835748792271e-05,
      "loss": 2.9847881317138674,
      "step": 2010
    },
    {
      "epoch": 2.927872417542588,
      "grad_norm": 30.761005401611328,
      "learning_rate": 5.13768115942029e-05,
      "loss": 3.8904369354248045,
      "step": 2020
    },
    {
      "epoch": 2.9423704240666906,
      "grad_norm": 24.463890075683594,
      "learning_rate": 5.113526570048309e-05,
      "loss": 3.360211944580078,
      "step": 2030
    },
    {
      "epoch": 2.9568684305907937,
      "grad_norm": 38.291629791259766,
      "learning_rate": 5.089371980676328e-05,
      "loss": 2.884589195251465,
      "step": 2040
    },
    {
      "epoch": 2.971366437114897,
      "grad_norm": 40.26332473754883,
      "learning_rate": 5.0652173913043474e-05,
      "loss": 3.067311477661133,
      "step": 2050
    },
    {
      "epoch": 2.9858644436389996,
      "grad_norm": 39.184356689453125,
      "learning_rate": 5.041062801932367e-05,
      "loss": 3.2425209045410157,
      "step": 2060
    },
    {
      "epoch": 3.0,
      "grad_norm": 54.05051040649414,
      "learning_rate": 5.016908212560387e-05,
      "loss": 3.712960052490234,
      "step": 2070
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.46423956751823425,
      "eval_runtime": 6.1176,
      "eval_samples_per_second": 9.154,
      "eval_steps_per_second": 1.144,
      "step": 2070
    },
    {
      "epoch": 3.0144980065241027,
      "grad_norm": 47.37404251098633,
      "learning_rate": 4.9927536231884056e-05,
      "loss": 3.104115104675293,
      "step": 2080
    },
    {
      "epoch": 3.028996013048206,
      "grad_norm": 34.25786209106445,
      "learning_rate": 4.9685990338164254e-05,
      "loss": 3.0405574798583985,
      "step": 2090
    },
    {
      "epoch": 3.043494019572309,
      "grad_norm": 41.59481430053711,
      "learning_rate": 4.9444444444444446e-05,
      "loss": 3.4512969970703127,
      "step": 2100
    },
    {
      "epoch": 3.0579920260964117,
      "grad_norm": 44.18028259277344,
      "learning_rate": 4.920289855072464e-05,
      "loss": 3.3819259643554687,
      "step": 2110
    },
    {
      "epoch": 3.0724900326205145,
      "grad_norm": 30.50547218322754,
      "learning_rate": 4.8961352657004836e-05,
      "loss": 3.2979049682617188,
      "step": 2120
    },
    {
      "epoch": 3.0869880391446176,
      "grad_norm": 66.72826385498047,
      "learning_rate": 4.871980676328503e-05,
      "loss": 4.126885986328125,
      "step": 2130
    },
    {
      "epoch": 3.1014860456687208,
      "grad_norm": 27.5252685546875,
      "learning_rate": 4.847826086956522e-05,
      "loss": 2.747353935241699,
      "step": 2140
    },
    {
      "epoch": 3.1159840521928235,
      "grad_norm": 36.9359016418457,
      "learning_rate": 4.823671497584542e-05,
      "loss": 3.035808563232422,
      "step": 2150
    },
    {
      "epoch": 3.130482058716926,
      "grad_norm": 79.57725524902344,
      "learning_rate": 4.799516908212561e-05,
      "loss": 3.5075931549072266,
      "step": 2160
    },
    {
      "epoch": 3.1449800652410294,
      "grad_norm": 23.18479347229004,
      "learning_rate": 4.77536231884058e-05,
      "loss": 2.965861701965332,
      "step": 2170
    },
    {
      "epoch": 3.1594780717651325,
      "grad_norm": 60.143585205078125,
      "learning_rate": 4.7512077294685994e-05,
      "loss": 4.023725128173828,
      "step": 2180
    },
    {
      "epoch": 3.1739760782892352,
      "grad_norm": 51.843414306640625,
      "learning_rate": 4.7270531400966185e-05,
      "loss": 3.5744338989257813,
      "step": 2190
    },
    {
      "epoch": 3.188474084813338,
      "grad_norm": 55.418861389160156,
      "learning_rate": 4.7028985507246384e-05,
      "loss": 3.2244834899902344,
      "step": 2200
    },
    {
      "epoch": 3.202972091337441,
      "grad_norm": 58.95329284667969,
      "learning_rate": 4.6787439613526576e-05,
      "loss": 3.2420238494873046,
      "step": 2210
    },
    {
      "epoch": 3.2174700978615443,
      "grad_norm": 44.365116119384766,
      "learning_rate": 4.654589371980677e-05,
      "loss": 3.5814998626708983,
      "step": 2220
    },
    {
      "epoch": 3.231968104385647,
      "grad_norm": 52.623836517333984,
      "learning_rate": 4.630434782608696e-05,
      "loss": 3.5055465698242188,
      "step": 2230
    },
    {
      "epoch": 3.2464661109097497,
      "grad_norm": 54.94391632080078,
      "learning_rate": 4.606280193236715e-05,
      "loss": 2.907185745239258,
      "step": 2240
    },
    {
      "epoch": 3.260964117433853,
      "grad_norm": 28.51628875732422,
      "learning_rate": 4.582125603864735e-05,
      "loss": 3.336173248291016,
      "step": 2250
    },
    {
      "epoch": 3.275462123957956,
      "grad_norm": 38.428829193115234,
      "learning_rate": 4.557971014492754e-05,
      "loss": 3.3089927673339843,
      "step": 2260
    },
    {
      "epoch": 3.2899601304820587,
      "grad_norm": 38.86235809326172,
      "learning_rate": 4.533816425120773e-05,
      "loss": 3.629587173461914,
      "step": 2270
    },
    {
      "epoch": 3.3044581370061614,
      "grad_norm": 59.401065826416016,
      "learning_rate": 4.5096618357487925e-05,
      "loss": 4.180054473876953,
      "step": 2280
    },
    {
      "epoch": 3.3189561435302646,
      "grad_norm": 37.10149383544922,
      "learning_rate": 4.4855072463768117e-05,
      "loss": 2.9403049468994142,
      "step": 2290
    },
    {
      "epoch": 3.3334541500543677,
      "grad_norm": 42.55136489868164,
      "learning_rate": 4.4613526570048315e-05,
      "loss": 3.4103111267089843,
      "step": 2300
    },
    {
      "epoch": 3.3479521565784705,
      "grad_norm": 53.69218063354492,
      "learning_rate": 4.437198067632851e-05,
      "loss": 3.174643707275391,
      "step": 2310
    },
    {
      "epoch": 3.362450163102573,
      "grad_norm": 43.839115142822266,
      "learning_rate": 4.41304347826087e-05,
      "loss": 3.1708019256591795,
      "step": 2320
    },
    {
      "epoch": 3.3769481696266763,
      "grad_norm": 33.245635986328125,
      "learning_rate": 4.388888888888889e-05,
      "loss": 3.1395374298095704,
      "step": 2330
    },
    {
      "epoch": 3.3914461761507795,
      "grad_norm": 29.049762725830078,
      "learning_rate": 4.364734299516908e-05,
      "loss": 4.227195739746094,
      "step": 2340
    },
    {
      "epoch": 3.405944182674882,
      "grad_norm": 77.19085693359375,
      "learning_rate": 4.3405797101449274e-05,
      "loss": 3.6114479064941407,
      "step": 2350
    },
    {
      "epoch": 3.420442189198985,
      "grad_norm": 40.499900817871094,
      "learning_rate": 4.316425120772947e-05,
      "loss": 3.355546569824219,
      "step": 2360
    },
    {
      "epoch": 3.434940195723088,
      "grad_norm": 51.300926208496094,
      "learning_rate": 4.2922705314009664e-05,
      "loss": 2.9470340728759767,
      "step": 2370
    },
    {
      "epoch": 3.449438202247191,
      "grad_norm": 43.87175369262695,
      "learning_rate": 4.2681159420289856e-05,
      "loss": 3.475551223754883,
      "step": 2380
    },
    {
      "epoch": 3.463936208771294,
      "grad_norm": 27.69055938720703,
      "learning_rate": 4.243961352657005e-05,
      "loss": 3.7229236602783202,
      "step": 2390
    },
    {
      "epoch": 3.4784342152953966,
      "grad_norm": 20.28310203552246,
      "learning_rate": 4.219806763285024e-05,
      "loss": 3.0216697692871093,
      "step": 2400
    },
    {
      "epoch": 3.4929322218195,
      "grad_norm": 60.839378356933594,
      "learning_rate": 4.195652173913044e-05,
      "loss": 3.5753505706787108,
      "step": 2410
    },
    {
      "epoch": 3.507430228343603,
      "grad_norm": 51.40070724487305,
      "learning_rate": 4.171497584541063e-05,
      "loss": 2.3699621200561523,
      "step": 2420
    },
    {
      "epoch": 3.5219282348677057,
      "grad_norm": 48.678218841552734,
      "learning_rate": 4.147342995169082e-05,
      "loss": 2.237067222595215,
      "step": 2430
    },
    {
      "epoch": 3.5364262413918084,
      "grad_norm": 68.0785140991211,
      "learning_rate": 4.123188405797101e-05,
      "loss": 3.7467254638671874,
      "step": 2440
    },
    {
      "epoch": 3.5509242479159115,
      "grad_norm": 44.07134246826172,
      "learning_rate": 4.0990338164251205e-05,
      "loss": 3.1750411987304688,
      "step": 2450
    },
    {
      "epoch": 3.5654222544400147,
      "grad_norm": 63.987850189208984,
      "learning_rate": 4.0748792270531403e-05,
      "loss": 4.309774780273438,
      "step": 2460
    },
    {
      "epoch": 3.5799202609641174,
      "grad_norm": 69.87521362304688,
      "learning_rate": 4.0507246376811595e-05,
      "loss": 3.8897159576416014,
      "step": 2470
    },
    {
      "epoch": 3.59441826748822,
      "grad_norm": 34.15118408203125,
      "learning_rate": 4.026570048309179e-05,
      "loss": 3.092060089111328,
      "step": 2480
    },
    {
      "epoch": 3.6089162740123233,
      "grad_norm": 71.10159301757812,
      "learning_rate": 4.002415458937198e-05,
      "loss": 3.4574665069580077,
      "step": 2490
    },
    {
      "epoch": 3.6234142805364264,
      "grad_norm": 37.73579025268555,
      "learning_rate": 3.978260869565217e-05,
      "loss": 3.398030090332031,
      "step": 2500
    },
    {
      "epoch": 3.637912287060529,
      "grad_norm": 46.70527267456055,
      "learning_rate": 3.954106280193237e-05,
      "loss": 3.4788330078125,
      "step": 2510
    },
    {
      "epoch": 3.652410293584632,
      "grad_norm": 48.76184844970703,
      "learning_rate": 3.929951690821256e-05,
      "loss": 3.7423633575439452,
      "step": 2520
    },
    {
      "epoch": 3.666908300108735,
      "grad_norm": 40.23674774169922,
      "learning_rate": 3.905797101449275e-05,
      "loss": 3.385464096069336,
      "step": 2530
    },
    {
      "epoch": 3.681406306632838,
      "grad_norm": 63.43309783935547,
      "learning_rate": 3.881642512077295e-05,
      "loss": 3.6296695709228515,
      "step": 2540
    },
    {
      "epoch": 3.695904313156941,
      "grad_norm": 29.975330352783203,
      "learning_rate": 3.857487922705314e-05,
      "loss": 2.411886787414551,
      "step": 2550
    },
    {
      "epoch": 3.7104023196810436,
      "grad_norm": 33.990577697753906,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 2.9424747467041015,
      "step": 2560
    },
    {
      "epoch": 3.7249003262051468,
      "grad_norm": 48.34798049926758,
      "learning_rate": 3.809178743961353e-05,
      "loss": 3.8303050994873047,
      "step": 2570
    },
    {
      "epoch": 3.73939833272925,
      "grad_norm": 21.951309204101562,
      "learning_rate": 3.7850241545893725e-05,
      "loss": 2.9493005752563475,
      "step": 2580
    },
    {
      "epoch": 3.7538963392533526,
      "grad_norm": 56.899635314941406,
      "learning_rate": 3.7608695652173917e-05,
      "loss": 3.075158882141113,
      "step": 2590
    },
    {
      "epoch": 3.7683943457774554,
      "grad_norm": 52.80284118652344,
      "learning_rate": 3.736714975845411e-05,
      "loss": 2.8288108825683596,
      "step": 2600
    },
    {
      "epoch": 3.7828923523015585,
      "grad_norm": 42.819217681884766,
      "learning_rate": 3.71256038647343e-05,
      "loss": 2.4956314086914064,
      "step": 2610
    },
    {
      "epoch": 3.7973903588256617,
      "grad_norm": 42.296173095703125,
      "learning_rate": 3.68840579710145e-05,
      "loss": 2.744287872314453,
      "step": 2620
    },
    {
      "epoch": 3.8118883653497644,
      "grad_norm": 28.224592208862305,
      "learning_rate": 3.664251207729469e-05,
      "loss": 3.885262298583984,
      "step": 2630
    },
    {
      "epoch": 3.826386371873867,
      "grad_norm": 43.404964447021484,
      "learning_rate": 3.640096618357488e-05,
      "loss": 3.4620506286621096,
      "step": 2640
    },
    {
      "epoch": 3.8408843783979703,
      "grad_norm": 45.9761962890625,
      "learning_rate": 3.6159420289855074e-05,
      "loss": 2.782696533203125,
      "step": 2650
    },
    {
      "epoch": 3.8553823849220734,
      "grad_norm": 54.05838394165039,
      "learning_rate": 3.5917874396135266e-05,
      "loss": 3.3177223205566406,
      "step": 2660
    },
    {
      "epoch": 3.869880391446176,
      "grad_norm": 42.38336944580078,
      "learning_rate": 3.5676328502415464e-05,
      "loss": 3.140911102294922,
      "step": 2670
    },
    {
      "epoch": 3.884378397970279,
      "grad_norm": 37.98568344116211,
      "learning_rate": 3.5434782608695656e-05,
      "loss": 3.006862831115723,
      "step": 2680
    },
    {
      "epoch": 3.898876404494382,
      "grad_norm": 50.27863693237305,
      "learning_rate": 3.519323671497585e-05,
      "loss": 2.679976463317871,
      "step": 2690
    },
    {
      "epoch": 3.913374411018485,
      "grad_norm": 36.56149673461914,
      "learning_rate": 3.495169082125604e-05,
      "loss": 2.8730201721191406,
      "step": 2700
    },
    {
      "epoch": 3.927872417542588,
      "grad_norm": 52.50223159790039,
      "learning_rate": 3.471014492753623e-05,
      "loss": 2.882431983947754,
      "step": 2710
    },
    {
      "epoch": 3.9423704240666906,
      "grad_norm": 51.335548400878906,
      "learning_rate": 3.446859903381643e-05,
      "loss": 3.465074920654297,
      "step": 2720
    },
    {
      "epoch": 3.9568684305907937,
      "grad_norm": 34.128013610839844,
      "learning_rate": 3.422705314009662e-05,
      "loss": 2.909779930114746,
      "step": 2730
    },
    {
      "epoch": 3.971366437114897,
      "grad_norm": 49.10036087036133,
      "learning_rate": 3.398550724637681e-05,
      "loss": 3.1126996994018556,
      "step": 2740
    },
    {
      "epoch": 3.9858644436389996,
      "grad_norm": 36.47686004638672,
      "learning_rate": 3.3743961352657005e-05,
      "loss": 3.448203277587891,
      "step": 2750
    },
    {
      "epoch": 4.0,
      "grad_norm": 33.80360794067383,
      "learning_rate": 3.35024154589372e-05,
      "loss": 3.3359375,
      "step": 2760
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.4562365710735321,
      "eval_runtime": 17.5994,
      "eval_samples_per_second": 3.182,
      "eval_steps_per_second": 0.398,
      "step": 2760
    },
    {
      "epoch": 4.014498006524103,
      "grad_norm": 38.8177375793457,
      "learning_rate": 3.3260869565217395e-05,
      "loss": 2.5573871612548826,
      "step": 2770
    },
    {
      "epoch": 4.028996013048205,
      "grad_norm": 43.11103057861328,
      "learning_rate": 3.301932367149759e-05,
      "loss": 3.2992210388183594,
      "step": 2780
    },
    {
      "epoch": 4.043494019572309,
      "grad_norm": 56.848480224609375,
      "learning_rate": 3.277777777777778e-05,
      "loss": 2.989871597290039,
      "step": 2790
    },
    {
      "epoch": 4.057992026096412,
      "grad_norm": 37.16334915161133,
      "learning_rate": 3.253623188405797e-05,
      "loss": 3.44771728515625,
      "step": 2800
    },
    {
      "epoch": 4.0724900326205145,
      "grad_norm": 56.230464935302734,
      "learning_rate": 3.229468599033816e-05,
      "loss": 3.957539367675781,
      "step": 2810
    },
    {
      "epoch": 4.086988039144618,
      "grad_norm": 21.304845809936523,
      "learning_rate": 3.205314009661836e-05,
      "loss": 2.825845718383789,
      "step": 2820
    },
    {
      "epoch": 4.101486045668721,
      "grad_norm": 45.06847381591797,
      "learning_rate": 3.181159420289855e-05,
      "loss": 3.109194755554199,
      "step": 2830
    },
    {
      "epoch": 4.1159840521928235,
      "grad_norm": 23.658123016357422,
      "learning_rate": 3.1570048309178744e-05,
      "loss": 2.752616310119629,
      "step": 2840
    },
    {
      "epoch": 4.130482058716926,
      "grad_norm": 20.395265579223633,
      "learning_rate": 3.1328502415458936e-05,
      "loss": 2.49576416015625,
      "step": 2850
    },
    {
      "epoch": 4.144980065241029,
      "grad_norm": 59.127349853515625,
      "learning_rate": 3.108695652173913e-05,
      "loss": 3.180251884460449,
      "step": 2860
    },
    {
      "epoch": 4.1594780717651325,
      "grad_norm": 44.46249771118164,
      "learning_rate": 3.0845410628019326e-05,
      "loss": 2.2334590911865235,
      "step": 2870
    },
    {
      "epoch": 4.173976078289235,
      "grad_norm": 30.323867797851562,
      "learning_rate": 3.060386473429952e-05,
      "loss": 2.3155149459838866,
      "step": 2880
    },
    {
      "epoch": 4.188474084813338,
      "grad_norm": 36.977699279785156,
      "learning_rate": 3.0362318840579713e-05,
      "loss": 3.354166030883789,
      "step": 2890
    },
    {
      "epoch": 4.2029720913374415,
      "grad_norm": 59.166351318359375,
      "learning_rate": 3.0120772946859905e-05,
      "loss": 3.65770263671875,
      "step": 2900
    },
    {
      "epoch": 4.217470097861544,
      "grad_norm": 42.278202056884766,
      "learning_rate": 2.9879227053140097e-05,
      "loss": 3.5780223846435546,
      "step": 2910
    },
    {
      "epoch": 4.231968104385647,
      "grad_norm": 46.633827209472656,
      "learning_rate": 2.963768115942029e-05,
      "loss": 2.727361488342285,
      "step": 2920
    },
    {
      "epoch": 4.24646611090975,
      "grad_norm": 37.50775146484375,
      "learning_rate": 2.9396135265700487e-05,
      "loss": 3.5634414672851564,
      "step": 2930
    },
    {
      "epoch": 4.260964117433852,
      "grad_norm": 80.06781005859375,
      "learning_rate": 2.915458937198068e-05,
      "loss": 3.130170440673828,
      "step": 2940
    },
    {
      "epoch": 4.275462123957956,
      "grad_norm": 57.591556549072266,
      "learning_rate": 2.891304347826087e-05,
      "loss": 2.243870162963867,
      "step": 2950
    },
    {
      "epoch": 4.289960130482059,
      "grad_norm": 39.20365524291992,
      "learning_rate": 2.8671497584541062e-05,
      "loss": 3.2684688568115234,
      "step": 2960
    },
    {
      "epoch": 4.304458137006161,
      "grad_norm": 31.059228897094727,
      "learning_rate": 2.8429951690821254e-05,
      "loss": 3.664652633666992,
      "step": 2970
    },
    {
      "epoch": 4.318956143530265,
      "grad_norm": 50.223995208740234,
      "learning_rate": 2.8188405797101452e-05,
      "loss": 3.1023334503173827,
      "step": 2980
    },
    {
      "epoch": 4.333454150054368,
      "grad_norm": 32.26329040527344,
      "learning_rate": 2.7946859903381644e-05,
      "loss": 3.0729509353637696,
      "step": 2990
    },
    {
      "epoch": 4.3479521565784705,
      "grad_norm": 54.038021087646484,
      "learning_rate": 2.7705314009661836e-05,
      "loss": 2.7129369735717774,
      "step": 3000
    },
    {
      "epoch": 4.362450163102573,
      "grad_norm": 38.99423599243164,
      "learning_rate": 2.7463768115942028e-05,
      "loss": 2.7501813888549806,
      "step": 3010
    },
    {
      "epoch": 4.376948169626676,
      "grad_norm": 29.15343475341797,
      "learning_rate": 2.7222222222222223e-05,
      "loss": 2.8839229583740233,
      "step": 3020
    },
    {
      "epoch": 4.3914461761507795,
      "grad_norm": 73.6739501953125,
      "learning_rate": 2.6980676328502418e-05,
      "loss": 2.8134954452514647,
      "step": 3030
    },
    {
      "epoch": 4.405944182674882,
      "grad_norm": 45.89149475097656,
      "learning_rate": 2.673913043478261e-05,
      "loss": 2.4591091156005858,
      "step": 3040
    },
    {
      "epoch": 4.420442189198985,
      "grad_norm": 31.317113876342773,
      "learning_rate": 2.6497584541062805e-05,
      "loss": 3.044528770446777,
      "step": 3050
    },
    {
      "epoch": 4.4349401957230885,
      "grad_norm": 47.90353775024414,
      "learning_rate": 2.6256038647342997e-05,
      "loss": 2.8303770065307616,
      "step": 3060
    },
    {
      "epoch": 4.449438202247191,
      "grad_norm": 57.917030334472656,
      "learning_rate": 2.601449275362319e-05,
      "loss": 3.1785251617431642,
      "step": 3070
    },
    {
      "epoch": 4.463936208771294,
      "grad_norm": 21.096529006958008,
      "learning_rate": 2.5772946859903384e-05,
      "loss": 2.274494743347168,
      "step": 3080
    },
    {
      "epoch": 4.478434215295397,
      "grad_norm": 56.620887756347656,
      "learning_rate": 2.553140096618358e-05,
      "loss": 2.647671127319336,
      "step": 3090
    },
    {
      "epoch": 4.492932221819499,
      "grad_norm": 33.706966400146484,
      "learning_rate": 2.528985507246377e-05,
      "loss": 2.534444808959961,
      "step": 3100
    },
    {
      "epoch": 4.507430228343603,
      "grad_norm": 56.68628692626953,
      "learning_rate": 2.5048309178743962e-05,
      "loss": 2.7690673828125,
      "step": 3110
    },
    {
      "epoch": 4.521928234867706,
      "grad_norm": 53.43891525268555,
      "learning_rate": 2.4806763285024157e-05,
      "loss": 1.9736810684204102,
      "step": 3120
    },
    {
      "epoch": 4.536426241391808,
      "grad_norm": 46.5354118347168,
      "learning_rate": 2.456521739130435e-05,
      "loss": 2.7956186294555665,
      "step": 3130
    },
    {
      "epoch": 4.550924247915912,
      "grad_norm": 31.08496856689453,
      "learning_rate": 2.432367149758454e-05,
      "loss": 2.3167160034179686,
      "step": 3140
    },
    {
      "epoch": 4.565422254440015,
      "grad_norm": 33.82973861694336,
      "learning_rate": 2.4082125603864736e-05,
      "loss": 3.266835403442383,
      "step": 3150
    },
    {
      "epoch": 4.579920260964117,
      "grad_norm": 46.80707931518555,
      "learning_rate": 2.3840579710144928e-05,
      "loss": 3.5418304443359374,
      "step": 3160
    },
    {
      "epoch": 4.59441826748822,
      "grad_norm": 36.81026840209961,
      "learning_rate": 2.3599033816425123e-05,
      "loss": 2.3871427536010743,
      "step": 3170
    },
    {
      "epoch": 4.608916274012323,
      "grad_norm": 24.534910202026367,
      "learning_rate": 2.3357487922705315e-05,
      "loss": 3.486624908447266,
      "step": 3180
    },
    {
      "epoch": 4.6234142805364264,
      "grad_norm": 35.61058807373047,
      "learning_rate": 2.3115942028985506e-05,
      "loss": 3.04370059967041,
      "step": 3190
    },
    {
      "epoch": 4.637912287060529,
      "grad_norm": 41.277828216552734,
      "learning_rate": 2.28743961352657e-05,
      "loss": 2.4568807601928713,
      "step": 3200
    },
    {
      "epoch": 4.652410293584632,
      "grad_norm": 37.66861343383789,
      "learning_rate": 2.2632850241545893e-05,
      "loss": 3.5434112548828125,
      "step": 3210
    },
    {
      "epoch": 4.6669083001087355,
      "grad_norm": 55.59501647949219,
      "learning_rate": 2.239130434782609e-05,
      "loss": 3.091164398193359,
      "step": 3220
    },
    {
      "epoch": 4.681406306632838,
      "grad_norm": 37.99943923950195,
      "learning_rate": 2.214975845410628e-05,
      "loss": 2.315280532836914,
      "step": 3230
    },
    {
      "epoch": 4.695904313156941,
      "grad_norm": 31.333948135375977,
      "learning_rate": 2.1908212560386472e-05,
      "loss": 3.2713008880615235,
      "step": 3240
    },
    {
      "epoch": 4.710402319681044,
      "grad_norm": 30.140323638916016,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 2.4228023529052733,
      "step": 3250
    },
    {
      "epoch": 4.724900326205146,
      "grad_norm": 69.66476440429688,
      "learning_rate": 2.1425120772946862e-05,
      "loss": 2.9011459350585938,
      "step": 3260
    },
    {
      "epoch": 4.73939833272925,
      "grad_norm": 31.404787063598633,
      "learning_rate": 2.1183574879227054e-05,
      "loss": 3.206926727294922,
      "step": 3270
    },
    {
      "epoch": 4.753896339253353,
      "grad_norm": 60.69810104370117,
      "learning_rate": 2.094202898550725e-05,
      "loss": 2.846786880493164,
      "step": 3280
    },
    {
      "epoch": 4.768394345777455,
      "grad_norm": 51.23883819580078,
      "learning_rate": 2.070048309178744e-05,
      "loss": 3.0598215103149413,
      "step": 3290
    },
    {
      "epoch": 4.782892352301559,
      "grad_norm": 50.26888656616211,
      "learning_rate": 2.0458937198067636e-05,
      "loss": 3.180216407775879,
      "step": 3300
    },
    {
      "epoch": 4.797390358825662,
      "grad_norm": 36.36679458618164,
      "learning_rate": 2.0217391304347828e-05,
      "loss": 2.8591283798217773,
      "step": 3310
    },
    {
      "epoch": 4.811888365349764,
      "grad_norm": 63.43571472167969,
      "learning_rate": 1.997584541062802e-05,
      "loss": 3.169123077392578,
      "step": 3320
    },
    {
      "epoch": 4.826386371873867,
      "grad_norm": 50.366336822509766,
      "learning_rate": 1.9734299516908215e-05,
      "loss": 2.3805797576904295,
      "step": 3330
    },
    {
      "epoch": 4.84088437839797,
      "grad_norm": 50.18330383300781,
      "learning_rate": 1.9492753623188406e-05,
      "loss": 3.231501007080078,
      "step": 3340
    },
    {
      "epoch": 4.855382384922073,
      "grad_norm": 28.253206253051758,
      "learning_rate": 1.92512077294686e-05,
      "loss": 2.8288827896118165,
      "step": 3350
    },
    {
      "epoch": 4.869880391446176,
      "grad_norm": 72.065185546875,
      "learning_rate": 1.9009661835748793e-05,
      "loss": 3.5866008758544923,
      "step": 3360
    },
    {
      "epoch": 4.884378397970279,
      "grad_norm": 44.85264587402344,
      "learning_rate": 1.8768115942028985e-05,
      "loss": 2.777370643615723,
      "step": 3370
    },
    {
      "epoch": 4.898876404494382,
      "grad_norm": 31.889493942260742,
      "learning_rate": 1.852657004830918e-05,
      "loss": 3.767107391357422,
      "step": 3380
    },
    {
      "epoch": 4.913374411018485,
      "grad_norm": 31.78213882446289,
      "learning_rate": 1.8285024154589372e-05,
      "loss": 2.79974365234375,
      "step": 3390
    },
    {
      "epoch": 4.927872417542588,
      "grad_norm": 57.98255920410156,
      "learning_rate": 1.8043478260869567e-05,
      "loss": 2.720087242126465,
      "step": 3400
    },
    {
      "epoch": 4.942370424066691,
      "grad_norm": 15.017969131469727,
      "learning_rate": 1.780193236714976e-05,
      "loss": 2.862108039855957,
      "step": 3410
    },
    {
      "epoch": 4.956868430590793,
      "grad_norm": 65.5716323852539,
      "learning_rate": 1.756038647342995e-05,
      "loss": 2.747179412841797,
      "step": 3420
    },
    {
      "epoch": 4.971366437114897,
      "grad_norm": 36.404476165771484,
      "learning_rate": 1.7318840579710146e-05,
      "loss": 2.9374458312988283,
      "step": 3430
    },
    {
      "epoch": 4.985864443639,
      "grad_norm": 47.91163635253906,
      "learning_rate": 1.7077294685990337e-05,
      "loss": 2.9378707885742186,
      "step": 3440
    },
    {
      "epoch": 5.0,
      "grad_norm": 41.447608947753906,
      "learning_rate": 1.6835748792270533e-05,
      "loss": 2.908673095703125,
      "step": 3450
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.42947033047676086,
      "eval_runtime": 20.0754,
      "eval_samples_per_second": 2.789,
      "eval_steps_per_second": 0.349,
      "step": 3450
    },
    {
      "epoch": 5.014498006524103,
      "grad_norm": 55.637939453125,
      "learning_rate": 1.6594202898550724e-05,
      "loss": 2.365453910827637,
      "step": 3460
    },
    {
      "epoch": 5.028996013048205,
      "grad_norm": 22.90374183654785,
      "learning_rate": 1.635265700483092e-05,
      "loss": 2.3711034774780275,
      "step": 3470
    },
    {
      "epoch": 5.043494019572309,
      "grad_norm": 42.60636520385742,
      "learning_rate": 1.6111111111111115e-05,
      "loss": 2.3237422943115233,
      "step": 3480
    },
    {
      "epoch": 5.057992026096412,
      "grad_norm": 50.490604400634766,
      "learning_rate": 1.5869565217391306e-05,
      "loss": 2.476716423034668,
      "step": 3490
    },
    {
      "epoch": 5.0724900326205145,
      "grad_norm": 54.374969482421875,
      "learning_rate": 1.5628019323671498e-05,
      "loss": 2.96311092376709,
      "step": 3500
    },
    {
      "epoch": 5.086988039144618,
      "grad_norm": 18.70736312866211,
      "learning_rate": 1.5386473429951693e-05,
      "loss": 2.654978370666504,
      "step": 3510
    },
    {
      "epoch": 5.101486045668721,
      "grad_norm": 54.168792724609375,
      "learning_rate": 1.5144927536231885e-05,
      "loss": 2.502501678466797,
      "step": 3520
    },
    {
      "epoch": 5.1159840521928235,
      "grad_norm": 38.15442657470703,
      "learning_rate": 1.4903381642512078e-05,
      "loss": 2.9817873001098634,
      "step": 3530
    },
    {
      "epoch": 5.130482058716926,
      "grad_norm": 58.18429946899414,
      "learning_rate": 1.4661835748792272e-05,
      "loss": 1.9490825653076171,
      "step": 3540
    },
    {
      "epoch": 5.144980065241029,
      "grad_norm": 41.037784576416016,
      "learning_rate": 1.4420289855072464e-05,
      "loss": 2.5604936599731447,
      "step": 3550
    },
    {
      "epoch": 5.1594780717651325,
      "grad_norm": 68.61034393310547,
      "learning_rate": 1.4178743961352659e-05,
      "loss": 3.0560157775878904,
      "step": 3560
    },
    {
      "epoch": 5.173976078289235,
      "grad_norm": 59.28275680541992,
      "learning_rate": 1.393719806763285e-05,
      "loss": 2.260776710510254,
      "step": 3570
    },
    {
      "epoch": 5.188474084813338,
      "grad_norm": 26.166181564331055,
      "learning_rate": 1.3695652173913042e-05,
      "loss": 2.0592117309570312,
      "step": 3580
    },
    {
      "epoch": 5.2029720913374415,
      "grad_norm": 52.45645523071289,
      "learning_rate": 1.3454106280193237e-05,
      "loss": 3.3053016662597656,
      "step": 3590
    },
    {
      "epoch": 5.217470097861544,
      "grad_norm": 36.958351135253906,
      "learning_rate": 1.321256038647343e-05,
      "loss": 2.836060905456543,
      "step": 3600
    },
    {
      "epoch": 5.231968104385647,
      "grad_norm": 70.35528564453125,
      "learning_rate": 1.2971014492753624e-05,
      "loss": 3.8392356872558593,
      "step": 3610
    },
    {
      "epoch": 5.24646611090975,
      "grad_norm": 73.23870086669922,
      "learning_rate": 1.2729468599033818e-05,
      "loss": 2.6702409744262696,
      "step": 3620
    },
    {
      "epoch": 5.260964117433852,
      "grad_norm": 44.49630355834961,
      "learning_rate": 1.2487922705314011e-05,
      "loss": 2.809039306640625,
      "step": 3630
    },
    {
      "epoch": 5.275462123957956,
      "grad_norm": 31.386930465698242,
      "learning_rate": 1.2246376811594205e-05,
      "loss": 3.5630687713623046,
      "step": 3640
    },
    {
      "epoch": 5.289960130482059,
      "grad_norm": 36.60596466064453,
      "learning_rate": 1.2004830917874396e-05,
      "loss": 2.9526262283325195,
      "step": 3650
    },
    {
      "epoch": 5.304458137006161,
      "grad_norm": 49.38300704956055,
      "learning_rate": 1.176328502415459e-05,
      "loss": 3.0672494888305666,
      "step": 3660
    },
    {
      "epoch": 5.318956143530265,
      "grad_norm": 51.39788055419922,
      "learning_rate": 1.1521739130434783e-05,
      "loss": 2.859596824645996,
      "step": 3670
    },
    {
      "epoch": 5.333454150054368,
      "grad_norm": 62.50394821166992,
      "learning_rate": 1.1280193236714977e-05,
      "loss": 2.462246894836426,
      "step": 3680
    },
    {
      "epoch": 5.3479521565784705,
      "grad_norm": 57.887451171875,
      "learning_rate": 1.1038647342995168e-05,
      "loss": 3.098234748840332,
      "step": 3690
    },
    {
      "epoch": 5.362450163102573,
      "grad_norm": 35.15022277832031,
      "learning_rate": 1.0797101449275362e-05,
      "loss": 2.82943172454834,
      "step": 3700
    },
    {
      "epoch": 5.376948169626676,
      "grad_norm": 31.80565071105957,
      "learning_rate": 1.0555555555555555e-05,
      "loss": 2.5534440994262697,
      "step": 3710
    },
    {
      "epoch": 5.3914461761507795,
      "grad_norm": 51.14878463745117,
      "learning_rate": 1.0314009661835749e-05,
      "loss": 3.286934661865234,
      "step": 3720
    },
    {
      "epoch": 5.405944182674882,
      "grad_norm": 65.72650909423828,
      "learning_rate": 1.0072463768115942e-05,
      "loss": 2.899969291687012,
      "step": 3730
    },
    {
      "epoch": 5.420442189198985,
      "grad_norm": 33.8945198059082,
      "learning_rate": 9.830917874396136e-06,
      "loss": 2.3307121276855467,
      "step": 3740
    },
    {
      "epoch": 5.4349401957230885,
      "grad_norm": 60.124610900878906,
      "learning_rate": 9.58937198067633e-06,
      "loss": 3.23609619140625,
      "step": 3750
    },
    {
      "epoch": 5.449438202247191,
      "grad_norm": 36.50619888305664,
      "learning_rate": 9.347826086956523e-06,
      "loss": 2.653668022155762,
      "step": 3760
    },
    {
      "epoch": 5.463936208771294,
      "grad_norm": 33.7532958984375,
      "learning_rate": 9.106280193236716e-06,
      "loss": 2.9939016342163085,
      "step": 3770
    },
    {
      "epoch": 5.478434215295397,
      "grad_norm": 35.533203125,
      "learning_rate": 8.864734299516908e-06,
      "loss": 2.662908363342285,
      "step": 3780
    },
    {
      "epoch": 5.492932221819499,
      "grad_norm": 80.38188934326172,
      "learning_rate": 8.623188405797101e-06,
      "loss": 2.123359489440918,
      "step": 3790
    },
    {
      "epoch": 5.507430228343603,
      "grad_norm": 58.80498123168945,
      "learning_rate": 8.381642512077295e-06,
      "loss": 2.2868112564086913,
      "step": 3800
    },
    {
      "epoch": 5.521928234867706,
      "grad_norm": 60.01420974731445,
      "learning_rate": 8.140096618357488e-06,
      "loss": 2.5027238845825197,
      "step": 3810
    },
    {
      "epoch": 5.536426241391808,
      "grad_norm": 39.53422546386719,
      "learning_rate": 7.898550724637682e-06,
      "loss": 2.4573135375976562,
      "step": 3820
    },
    {
      "epoch": 5.550924247915912,
      "grad_norm": 86.68050384521484,
      "learning_rate": 7.657004830917875e-06,
      "loss": 3.08843994140625,
      "step": 3830
    },
    {
      "epoch": 5.565422254440015,
      "grad_norm": 55.49795913696289,
      "learning_rate": 7.415458937198068e-06,
      "loss": 3.105560302734375,
      "step": 3840
    },
    {
      "epoch": 5.579920260964117,
      "grad_norm": 48.278446197509766,
      "learning_rate": 7.173913043478261e-06,
      "loss": 2.9628957748413085,
      "step": 3850
    },
    {
      "epoch": 5.59441826748822,
      "grad_norm": 54.09696960449219,
      "learning_rate": 6.932367149758455e-06,
      "loss": 3.0336835861206053,
      "step": 3860
    },
    {
      "epoch": 5.608916274012323,
      "grad_norm": 35.886573791503906,
      "learning_rate": 6.690821256038647e-06,
      "loss": 2.991535758972168,
      "step": 3870
    },
    {
      "epoch": 5.6234142805364264,
      "grad_norm": 52.51179885864258,
      "learning_rate": 6.449275362318841e-06,
      "loss": 2.7944116592407227,
      "step": 3880
    },
    {
      "epoch": 5.637912287060529,
      "grad_norm": 68.60572814941406,
      "learning_rate": 6.207729468599034e-06,
      "loss": 2.6559099197387694,
      "step": 3890
    },
    {
      "epoch": 5.652410293584632,
      "grad_norm": 37.48847198486328,
      "learning_rate": 5.9661835748792275e-06,
      "loss": 2.991391181945801,
      "step": 3900
    },
    {
      "epoch": 5.6669083001087355,
      "grad_norm": 63.748878479003906,
      "learning_rate": 5.724637681159421e-06,
      "loss": 3.378229522705078,
      "step": 3910
    },
    {
      "epoch": 5.681406306632838,
      "grad_norm": 40.61078643798828,
      "learning_rate": 5.483091787439614e-06,
      "loss": 2.928306770324707,
      "step": 3920
    },
    {
      "epoch": 5.695904313156941,
      "grad_norm": 54.84019470214844,
      "learning_rate": 5.241545893719807e-06,
      "loss": 2.5125484466552734,
      "step": 3930
    },
    {
      "epoch": 5.710402319681044,
      "grad_norm": 22.89272689819336,
      "learning_rate": 5e-06,
      "loss": 3.168906593322754,
      "step": 3940
    },
    {
      "epoch": 5.724900326205146,
      "grad_norm": 31.758647918701172,
      "learning_rate": 4.758454106280193e-06,
      "loss": 2.3090866088867186,
      "step": 3950
    },
    {
      "epoch": 5.73939833272925,
      "grad_norm": 51.20511245727539,
      "learning_rate": 4.516908212560387e-06,
      "loss": 3.115652656555176,
      "step": 3960
    },
    {
      "epoch": 5.753896339253353,
      "grad_norm": 63.10453796386719,
      "learning_rate": 4.27536231884058e-06,
      "loss": 3.2022228240966797,
      "step": 3970
    },
    {
      "epoch": 5.768394345777455,
      "grad_norm": 36.68155288696289,
      "learning_rate": 4.033816425120773e-06,
      "loss": 2.41012020111084,
      "step": 3980
    },
    {
      "epoch": 5.782892352301559,
      "grad_norm": 40.250213623046875,
      "learning_rate": 3.792270531400966e-06,
      "loss": 3.2422840118408205,
      "step": 3990
    },
    {
      "epoch": 5.797390358825662,
      "grad_norm": 47.25126266479492,
      "learning_rate": 3.55072463768116e-06,
      "loss": 2.2729867935180663,
      "step": 4000
    },
    {
      "epoch": 5.811888365349764,
      "grad_norm": 39.81022644042969,
      "learning_rate": 3.3091787439613524e-06,
      "loss": 1.9436485290527343,
      "step": 4010
    },
    {
      "epoch": 5.826386371873867,
      "grad_norm": 61.186561584472656,
      "learning_rate": 3.067632850241546e-06,
      "loss": 3.0088306427001954,
      "step": 4020
    },
    {
      "epoch": 5.84088437839797,
      "grad_norm": 55.50312042236328,
      "learning_rate": 2.8260869565217393e-06,
      "loss": 2.932918739318848,
      "step": 4030
    },
    {
      "epoch": 5.855382384922073,
      "grad_norm": 47.03285598754883,
      "learning_rate": 2.5845410628019323e-06,
      "loss": 2.7481754302978514,
      "step": 4040
    },
    {
      "epoch": 5.869880391446176,
      "grad_norm": 29.12235450744629,
      "learning_rate": 2.3429951690821258e-06,
      "loss": 1.8580402374267577,
      "step": 4050
    },
    {
      "epoch": 5.884378397970279,
      "grad_norm": 47.72272872924805,
      "learning_rate": 2.101449275362319e-06,
      "loss": 2.4999696731567385,
      "step": 4060
    },
    {
      "epoch": 5.898876404494382,
      "grad_norm": 56.97807312011719,
      "learning_rate": 1.859903381642512e-06,
      "loss": 2.1282087326049806,
      "step": 4070
    },
    {
      "epoch": 5.913374411018485,
      "grad_norm": 38.566959381103516,
      "learning_rate": 1.6183574879227053e-06,
      "loss": 2.379653549194336,
      "step": 4080
    },
    {
      "epoch": 5.927872417542588,
      "grad_norm": 45.163490295410156,
      "learning_rate": 1.3768115942028985e-06,
      "loss": 3.3187015533447264,
      "step": 4090
    },
    {
      "epoch": 5.942370424066691,
      "grad_norm": 59.52109909057617,
      "learning_rate": 1.1352657004830917e-06,
      "loss": 2.623271179199219,
      "step": 4100
    },
    {
      "epoch": 5.956868430590793,
      "grad_norm": 37.886051177978516,
      "learning_rate": 8.937198067632852e-07,
      "loss": 2.687204360961914,
      "step": 4110
    },
    {
      "epoch": 5.971366437114897,
      "grad_norm": 37.872188568115234,
      "learning_rate": 6.521739130434782e-07,
      "loss": 3.1637807846069337,
      "step": 4120
    },
    {
      "epoch": 5.985864443639,
      "grad_norm": 29.76531982421875,
      "learning_rate": 4.1062801932367154e-07,
      "loss": 2.1595659255981445,
      "step": 4130
    },
    {
      "epoch": 6.0,
      "grad_norm": 35.68534851074219,
      "learning_rate": 1.6908212560386475e-07,
      "loss": 2.739679718017578,
      "step": 4140
    }
  ],
  "logging_steps": 10,
  "max_steps": 4140,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.16144483581952e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
