{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 6.0,
  "eval_steps": 500,
  "global_step": 3348,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01794124243103835,
      "grad_norm": 58.948081970214844,
      "learning_rate": 9.985065710872163e-05,
      "loss": 82.6583984375,
      "step": 10
    },
    {
      "epoch": 0.0358824848620767,
      "grad_norm": 62.32081604003906,
      "learning_rate": 9.955197132616488e-05,
      "loss": 76.6780517578125,
      "step": 20
    },
    {
      "epoch": 0.05382372729311505,
      "grad_norm": 61.594932556152344,
      "learning_rate": 9.925328554360813e-05,
      "loss": 70.2847412109375,
      "step": 30
    },
    {
      "epoch": 0.0717649697241534,
      "grad_norm": 77.35279846191406,
      "learning_rate": 9.895459976105139e-05,
      "loss": 60.6589111328125,
      "step": 40
    },
    {
      "epoch": 0.08970621215519174,
      "grad_norm": 88.54511260986328,
      "learning_rate": 9.865591397849462e-05,
      "loss": 47.575738525390626,
      "step": 50
    },
    {
      "epoch": 0.1076474545862301,
      "grad_norm": 156.15757751464844,
      "learning_rate": 9.835722819593787e-05,
      "loss": 34.13216857910156,
      "step": 60
    },
    {
      "epoch": 0.12558869701726844,
      "grad_norm": 88.87764739990234,
      "learning_rate": 9.805854241338113e-05,
      "loss": 28.441033935546876,
      "step": 70
    },
    {
      "epoch": 0.1435299394483068,
      "grad_norm": 70.76599884033203,
      "learning_rate": 9.775985663082438e-05,
      "loss": 18.721127319335938,
      "step": 80
    },
    {
      "epoch": 0.16147118187934514,
      "grad_norm": 47.46059799194336,
      "learning_rate": 9.746117084826763e-05,
      "loss": 15.501004028320313,
      "step": 90
    },
    {
      "epoch": 0.17941242431038348,
      "grad_norm": 61.17384338378906,
      "learning_rate": 9.716248506571087e-05,
      "loss": 12.353428649902344,
      "step": 100
    },
    {
      "epoch": 0.19735366674142185,
      "grad_norm": 47.802642822265625,
      "learning_rate": 9.68936678614098e-05,
      "loss": 12.757640838623047,
      "step": 110
    },
    {
      "epoch": 0.2152949091724602,
      "grad_norm": 58.76526641845703,
      "learning_rate": 9.659498207885304e-05,
      "loss": 11.579010772705079,
      "step": 120
    },
    {
      "epoch": 0.23323615160349853,
      "grad_norm": 84.06797790527344,
      "learning_rate": 9.62962962962963e-05,
      "loss": 8.826779174804688,
      "step": 130
    },
    {
      "epoch": 0.25117739403453687,
      "grad_norm": 46.432861328125,
      "learning_rate": 9.599761051373956e-05,
      "loss": 9.145578002929687,
      "step": 140
    },
    {
      "epoch": 0.26911863646557527,
      "grad_norm": 113.28273010253906,
      "learning_rate": 9.56989247311828e-05,
      "loss": 9.311868286132812,
      "step": 150
    },
    {
      "epoch": 0.2870598788966136,
      "grad_norm": 71.66376495361328,
      "learning_rate": 9.540023894862605e-05,
      "loss": 8.9428955078125,
      "step": 160
    },
    {
      "epoch": 0.30500112132765195,
      "grad_norm": 69.18794250488281,
      "learning_rate": 9.51015531660693e-05,
      "loss": 8.153229522705079,
      "step": 170
    },
    {
      "epoch": 0.3229423637586903,
      "grad_norm": 56.17432403564453,
      "learning_rate": 9.480286738351255e-05,
      "loss": 7.435529327392578,
      "step": 180
    },
    {
      "epoch": 0.3408836061897286,
      "grad_norm": 40.04631423950195,
      "learning_rate": 9.45041816009558e-05,
      "loss": 9.041358947753906,
      "step": 190
    },
    {
      "epoch": 0.35882484862076697,
      "grad_norm": 37.78630828857422,
      "learning_rate": 9.420549581839904e-05,
      "loss": 8.314459228515625,
      "step": 200
    },
    {
      "epoch": 0.37676609105180536,
      "grad_norm": 44.517982482910156,
      "learning_rate": 9.39068100358423e-05,
      "loss": 6.9975639343261715,
      "step": 210
    },
    {
      "epoch": 0.3947073334828437,
      "grad_norm": 42.95568084716797,
      "learning_rate": 9.360812425328555e-05,
      "loss": 7.628817749023438,
      "step": 220
    },
    {
      "epoch": 0.41264857591388204,
      "grad_norm": 38.48308563232422,
      "learning_rate": 9.33094384707288e-05,
      "loss": 6.832013702392578,
      "step": 230
    },
    {
      "epoch": 0.4305898183449204,
      "grad_norm": 33.097198486328125,
      "learning_rate": 9.301075268817204e-05,
      "loss": 6.484679412841797,
      "step": 240
    },
    {
      "epoch": 0.4485310607759587,
      "grad_norm": 45.85905456542969,
      "learning_rate": 9.27120669056153e-05,
      "loss": 6.914672088623047,
      "step": 250
    },
    {
      "epoch": 0.46647230320699706,
      "grad_norm": 60.58846664428711,
      "learning_rate": 9.241338112305855e-05,
      "loss": 6.335167312622071,
      "step": 260
    },
    {
      "epoch": 0.48441354563803546,
      "grad_norm": 38.47651290893555,
      "learning_rate": 9.21146953405018e-05,
      "loss": 7.437565612792969,
      "step": 270
    },
    {
      "epoch": 0.5023547880690737,
      "grad_norm": 36.62727355957031,
      "learning_rate": 9.181600955794504e-05,
      "loss": 6.95198974609375,
      "step": 280
    },
    {
      "epoch": 0.5202960305001121,
      "grad_norm": 45.68613052368164,
      "learning_rate": 9.15173237753883e-05,
      "loss": 6.742987060546875,
      "step": 290
    },
    {
      "epoch": 0.5382372729311505,
      "grad_norm": 39.52165603637695,
      "learning_rate": 9.121863799283154e-05,
      "loss": 6.648194885253906,
      "step": 300
    },
    {
      "epoch": 0.5561785153621889,
      "grad_norm": 54.81141662597656,
      "learning_rate": 9.09199522102748e-05,
      "loss": 6.899224853515625,
      "step": 310
    },
    {
      "epoch": 0.5741197577932272,
      "grad_norm": 42.89790725708008,
      "learning_rate": 9.062126642771805e-05,
      "loss": 5.380582809448242,
      "step": 320
    },
    {
      "epoch": 0.5920610002242656,
      "grad_norm": 29.812244415283203,
      "learning_rate": 9.032258064516129e-05,
      "loss": 4.333689498901367,
      "step": 330
    },
    {
      "epoch": 0.6100022426553039,
      "grad_norm": 38.31150436401367,
      "learning_rate": 9.002389486260454e-05,
      "loss": 7.212512969970703,
      "step": 340
    },
    {
      "epoch": 0.6279434850863422,
      "grad_norm": 60.67488098144531,
      "learning_rate": 8.972520908004779e-05,
      "loss": 6.319738388061523,
      "step": 350
    },
    {
      "epoch": 0.6458847275173806,
      "grad_norm": 47.6341667175293,
      "learning_rate": 8.942652329749104e-05,
      "loss": 6.325899505615235,
      "step": 360
    },
    {
      "epoch": 0.6638259699484189,
      "grad_norm": 83.54837036132812,
      "learning_rate": 8.912783751493429e-05,
      "loss": 7.311550140380859,
      "step": 370
    },
    {
      "epoch": 0.6817672123794573,
      "grad_norm": 68.79307556152344,
      "learning_rate": 8.882915173237754e-05,
      "loss": 5.525005722045899,
      "step": 380
    },
    {
      "epoch": 0.6997084548104956,
      "grad_norm": 48.90336227416992,
      "learning_rate": 8.85304659498208e-05,
      "loss": 6.151713562011719,
      "step": 390
    },
    {
      "epoch": 0.7176496972415339,
      "grad_norm": 42.37086486816406,
      "learning_rate": 8.823178016726405e-05,
      "loss": 6.176874160766602,
      "step": 400
    },
    {
      "epoch": 0.7355909396725723,
      "grad_norm": 40.20478057861328,
      "learning_rate": 8.79330943847073e-05,
      "loss": 6.467395782470703,
      "step": 410
    },
    {
      "epoch": 0.7535321821036107,
      "grad_norm": 44.11841583251953,
      "learning_rate": 8.763440860215054e-05,
      "loss": 7.57818374633789,
      "step": 420
    },
    {
      "epoch": 0.7714734245346491,
      "grad_norm": 42.01285171508789,
      "learning_rate": 8.733572281959379e-05,
      "loss": 6.443003845214844,
      "step": 430
    },
    {
      "epoch": 0.7894146669656874,
      "grad_norm": 56.770233154296875,
      "learning_rate": 8.703703703703704e-05,
      "loss": 6.450236511230469,
      "step": 440
    },
    {
      "epoch": 0.8073559093967257,
      "grad_norm": 39.89718246459961,
      "learning_rate": 8.673835125448029e-05,
      "loss": 5.370693588256836,
      "step": 450
    },
    {
      "epoch": 0.8252971518277641,
      "grad_norm": 27.582014083862305,
      "learning_rate": 8.643966547192354e-05,
      "loss": 5.449821472167969,
      "step": 460
    },
    {
      "epoch": 0.8432383942588024,
      "grad_norm": 40.23586654663086,
      "learning_rate": 8.614097968936678e-05,
      "loss": 6.611750793457031,
      "step": 470
    },
    {
      "epoch": 0.8611796366898408,
      "grad_norm": 63.84882736206055,
      "learning_rate": 8.584229390681004e-05,
      "loss": 7.054838562011719,
      "step": 480
    },
    {
      "epoch": 0.8791208791208791,
      "grad_norm": 58.39216232299805,
      "learning_rate": 8.55436081242533e-05,
      "loss": 5.404143524169922,
      "step": 490
    },
    {
      "epoch": 0.8970621215519174,
      "grad_norm": 39.80171203613281,
      "learning_rate": 8.524492234169655e-05,
      "loss": 7.107543182373047,
      "step": 500
    },
    {
      "epoch": 0.9150033639829558,
      "grad_norm": 67.11798858642578,
      "learning_rate": 8.494623655913979e-05,
      "loss": 6.2307586669921875,
      "step": 510
    },
    {
      "epoch": 0.9329446064139941,
      "grad_norm": 60.69375228881836,
      "learning_rate": 8.464755077658304e-05,
      "loss": 6.230491638183594,
      "step": 520
    },
    {
      "epoch": 0.9508858488450325,
      "grad_norm": 38.7950553894043,
      "learning_rate": 8.434886499402629e-05,
      "loss": 6.550897216796875,
      "step": 530
    },
    {
      "epoch": 0.9688270912760709,
      "grad_norm": 54.38425064086914,
      "learning_rate": 8.405017921146954e-05,
      "loss": 5.810834121704102,
      "step": 540
    },
    {
      "epoch": 0.9867683337071093,
      "grad_norm": 52.0772590637207,
      "learning_rate": 8.375149342891279e-05,
      "loss": 6.277215576171875,
      "step": 550
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.7939699292182922,
      "eval_runtime": 124.2659,
      "eval_samples_per_second": 8.973,
      "eval_steps_per_second": 1.127,
      "step": 558
    },
    {
      "epoch": 1.0035882484862078,
      "grad_norm": 37.436702728271484,
      "learning_rate": 8.345280764635603e-05,
      "loss": 5.2713470458984375,
      "step": 560
    },
    {
      "epoch": 1.021529490917246,
      "grad_norm": 51.401824951171875,
      "learning_rate": 8.315412186379928e-05,
      "loss": 5.744375610351563,
      "step": 570
    },
    {
      "epoch": 1.0394707333482844,
      "grad_norm": 33.158233642578125,
      "learning_rate": 8.285543608124253e-05,
      "loss": 5.228097915649414,
      "step": 580
    },
    {
      "epoch": 1.0574119757793228,
      "grad_norm": 26.649333953857422,
      "learning_rate": 8.255675029868578e-05,
      "loss": 4.941537475585937,
      "step": 590
    },
    {
      "epoch": 1.0753532182103611,
      "grad_norm": 46.0879020690918,
      "learning_rate": 8.225806451612904e-05,
      "loss": 4.218491744995117,
      "step": 600
    },
    {
      "epoch": 1.0932944606413995,
      "grad_norm": 44.489315032958984,
      "learning_rate": 8.195937873357229e-05,
      "loss": 5.121236801147461,
      "step": 610
    },
    {
      "epoch": 1.1112357030724378,
      "grad_norm": 77.01342010498047,
      "learning_rate": 8.166069295101554e-05,
      "loss": 4.878796768188477,
      "step": 620
    },
    {
      "epoch": 1.1291769455034761,
      "grad_norm": 56.86571502685547,
      "learning_rate": 8.136200716845879e-05,
      "loss": 5.377603912353516,
      "step": 630
    },
    {
      "epoch": 1.1471181879345145,
      "grad_norm": 64.24784851074219,
      "learning_rate": 8.106332138590204e-05,
      "loss": 5.758558654785157,
      "step": 640
    },
    {
      "epoch": 1.1650594303655528,
      "grad_norm": 58.77791213989258,
      "learning_rate": 8.076463560334528e-05,
      "loss": 6.380055999755859,
      "step": 650
    },
    {
      "epoch": 1.1830006727965912,
      "grad_norm": 49.48268508911133,
      "learning_rate": 8.046594982078853e-05,
      "loss": 4.546796417236328,
      "step": 660
    },
    {
      "epoch": 1.2009419152276295,
      "grad_norm": 58.14021301269531,
      "learning_rate": 8.016726403823178e-05,
      "loss": 5.059477233886719,
      "step": 670
    },
    {
      "epoch": 1.2188831576586678,
      "grad_norm": 48.445335388183594,
      "learning_rate": 7.986857825567503e-05,
      "loss": 5.701578140258789,
      "step": 680
    },
    {
      "epoch": 1.2368244000897062,
      "grad_norm": 59.901893615722656,
      "learning_rate": 7.956989247311829e-05,
      "loss": 5.4090625762939455,
      "step": 690
    },
    {
      "epoch": 1.2547656425207445,
      "grad_norm": 47.903865814208984,
      "learning_rate": 7.927120669056152e-05,
      "loss": 4.48564338684082,
      "step": 700
    },
    {
      "epoch": 1.2727068849517829,
      "grad_norm": 45.096778869628906,
      "learning_rate": 7.897252090800479e-05,
      "loss": 4.367104339599609,
      "step": 710
    },
    {
      "epoch": 1.2906481273828212,
      "grad_norm": 83.25447845458984,
      "learning_rate": 7.867383512544804e-05,
      "loss": 5.7276042938232425,
      "step": 720
    },
    {
      "epoch": 1.3085893698138595,
      "grad_norm": 51.659568786621094,
      "learning_rate": 7.837514934289129e-05,
      "loss": 4.952676391601562,
      "step": 730
    },
    {
      "epoch": 1.3265306122448979,
      "grad_norm": 38.99289321899414,
      "learning_rate": 7.807646356033453e-05,
      "loss": 5.936967849731445,
      "step": 740
    },
    {
      "epoch": 1.3444718546759362,
      "grad_norm": 55.28466796875,
      "learning_rate": 7.777777777777778e-05,
      "loss": 5.220784378051758,
      "step": 750
    },
    {
      "epoch": 1.3624130971069746,
      "grad_norm": 49.621028900146484,
      "learning_rate": 7.747909199522103e-05,
      "loss": 5.63238639831543,
      "step": 760
    },
    {
      "epoch": 1.380354339538013,
      "grad_norm": 44.875465393066406,
      "learning_rate": 7.718040621266428e-05,
      "loss": 5.060157012939453,
      "step": 770
    },
    {
      "epoch": 1.3982955819690512,
      "grad_norm": 91.86054992675781,
      "learning_rate": 7.688172043010752e-05,
      "loss": 4.657062149047851,
      "step": 780
    },
    {
      "epoch": 1.4162368244000896,
      "grad_norm": 60.253841400146484,
      "learning_rate": 7.658303464755077e-05,
      "loss": 6.24052619934082,
      "step": 790
    },
    {
      "epoch": 1.434178066831128,
      "grad_norm": 52.666297912597656,
      "learning_rate": 7.628434886499403e-05,
      "loss": 4.328139877319336,
      "step": 800
    },
    {
      "epoch": 1.4521193092621665,
      "grad_norm": 49.088253021240234,
      "learning_rate": 7.598566308243728e-05,
      "loss": 4.717316055297852,
      "step": 810
    },
    {
      "epoch": 1.4700605516932048,
      "grad_norm": 60.9149169921875,
      "learning_rate": 7.568697729988053e-05,
      "loss": 5.106869888305664,
      "step": 820
    },
    {
      "epoch": 1.4880017941242432,
      "grad_norm": 92.98571014404297,
      "learning_rate": 7.538829151732378e-05,
      "loss": 5.948272323608398,
      "step": 830
    },
    {
      "epoch": 1.5059430365552815,
      "grad_norm": 60.99308776855469,
      "learning_rate": 7.508960573476703e-05,
      "loss": 4.784518432617188,
      "step": 840
    },
    {
      "epoch": 1.5238842789863198,
      "grad_norm": 52.82758331298828,
      "learning_rate": 7.479091995221028e-05,
      "loss": 4.666561889648437,
      "step": 850
    },
    {
      "epoch": 1.5418255214173582,
      "grad_norm": 46.587684631347656,
      "learning_rate": 7.449223416965353e-05,
      "loss": 4.904068374633789,
      "step": 860
    },
    {
      "epoch": 1.5597667638483965,
      "grad_norm": 39.25101852416992,
      "learning_rate": 7.419354838709677e-05,
      "loss": 5.119340515136718,
      "step": 870
    },
    {
      "epoch": 1.5777080062794349,
      "grad_norm": 33.38090133666992,
      "learning_rate": 7.389486260454002e-05,
      "loss": 5.006039428710937,
      "step": 880
    },
    {
      "epoch": 1.5956492487104732,
      "grad_norm": 39.74276351928711,
      "learning_rate": 7.359617682198328e-05,
      "loss": 4.962881469726563,
      "step": 890
    },
    {
      "epoch": 1.6135904911415115,
      "grad_norm": 45.35476303100586,
      "learning_rate": 7.329749103942653e-05,
      "loss": 4.730079650878906,
      "step": 900
    },
    {
      "epoch": 1.6315317335725499,
      "grad_norm": 27.192642211914062,
      "learning_rate": 7.299880525686978e-05,
      "loss": 3.6201133728027344,
      "step": 910
    },
    {
      "epoch": 1.6494729760035882,
      "grad_norm": 46.925228118896484,
      "learning_rate": 7.270011947431302e-05,
      "loss": 4.973274612426758,
      "step": 920
    },
    {
      "epoch": 1.6674142184346266,
      "grad_norm": 58.01185989379883,
      "learning_rate": 7.240143369175627e-05,
      "loss": 4.666497421264649,
      "step": 930
    },
    {
      "epoch": 1.685355460865665,
      "grad_norm": 35.43035888671875,
      "learning_rate": 7.210274790919953e-05,
      "loss": 5.455519104003907,
      "step": 940
    },
    {
      "epoch": 1.7032967032967035,
      "grad_norm": 48.94633865356445,
      "learning_rate": 7.180406212664278e-05,
      "loss": 5.745280075073242,
      "step": 950
    },
    {
      "epoch": 1.7212379457277418,
      "grad_norm": 50.603702545166016,
      "learning_rate": 7.150537634408602e-05,
      "loss": 4.716108322143555,
      "step": 960
    },
    {
      "epoch": 1.7391791881587801,
      "grad_norm": 43.49995040893555,
      "learning_rate": 7.120669056152927e-05,
      "loss": 5.031351470947266,
      "step": 970
    },
    {
      "epoch": 1.7571204305898185,
      "grad_norm": 52.03779220581055,
      "learning_rate": 7.090800477897253e-05,
      "loss": 5.494896316528321,
      "step": 980
    },
    {
      "epoch": 1.7750616730208568,
      "grad_norm": 40.01875686645508,
      "learning_rate": 7.060931899641578e-05,
      "loss": 3.7049285888671877,
      "step": 990
    },
    {
      "epoch": 1.7930029154518952,
      "grad_norm": 46.81450271606445,
      "learning_rate": 7.031063321385903e-05,
      "loss": 5.053987121582031,
      "step": 1000
    },
    {
      "epoch": 1.8109441578829335,
      "grad_norm": 53.42730712890625,
      "learning_rate": 7.001194743130227e-05,
      "loss": 4.286854934692383,
      "step": 1010
    },
    {
      "epoch": 1.8288854003139718,
      "grad_norm": 41.611358642578125,
      "learning_rate": 6.971326164874552e-05,
      "loss": 4.628773880004883,
      "step": 1020
    },
    {
      "epoch": 1.8468266427450102,
      "grad_norm": 63.39658737182617,
      "learning_rate": 6.941457586618877e-05,
      "loss": 5.245030212402344,
      "step": 1030
    },
    {
      "epoch": 1.8647678851760485,
      "grad_norm": 36.91254425048828,
      "learning_rate": 6.911589008363202e-05,
      "loss": 4.338234329223633,
      "step": 1040
    },
    {
      "epoch": 1.8827091276070869,
      "grad_norm": 54.2412109375,
      "learning_rate": 6.881720430107527e-05,
      "loss": 4.709703063964843,
      "step": 1050
    },
    {
      "epoch": 1.9006503700381252,
      "grad_norm": 26.731971740722656,
      "learning_rate": 6.851851851851852e-05,
      "loss": 4.588542556762695,
      "step": 1060
    },
    {
      "epoch": 1.9185916124691635,
      "grad_norm": 59.55648422241211,
      "learning_rate": 6.821983273596178e-05,
      "loss": 4.249917984008789,
      "step": 1070
    },
    {
      "epoch": 1.9365328549002019,
      "grad_norm": 29.694414138793945,
      "learning_rate": 6.792114695340503e-05,
      "loss": 5.638947677612305,
      "step": 1080
    },
    {
      "epoch": 1.9544740973312402,
      "grad_norm": 53.63369369506836,
      "learning_rate": 6.762246117084828e-05,
      "loss": 4.91778793334961,
      "step": 1090
    },
    {
      "epoch": 1.9724153397622786,
      "grad_norm": 40.54218292236328,
      "learning_rate": 6.732377538829152e-05,
      "loss": 4.012060928344726,
      "step": 1100
    },
    {
      "epoch": 1.990356582193317,
      "grad_norm": 56.48468017578125,
      "learning_rate": 6.702508960573477e-05,
      "loss": 4.6409423828125,
      "step": 1110
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.6716061234474182,
      "eval_runtime": 227.3457,
      "eval_samples_per_second": 4.904,
      "eval_steps_per_second": 0.616,
      "step": 1116
    },
    {
      "epoch": 2.0071764969724155,
      "grad_norm": 33.33372116088867,
      "learning_rate": 6.672640382317802e-05,
      "loss": 4.178876876831055,
      "step": 1120
    },
    {
      "epoch": 2.025117739403454,
      "grad_norm": 38.89065170288086,
      "learning_rate": 6.642771804062127e-05,
      "loss": 4.018807983398437,
      "step": 1130
    },
    {
      "epoch": 2.043058981834492,
      "grad_norm": 56.980438232421875,
      "learning_rate": 6.612903225806452e-05,
      "loss": 4.927982330322266,
      "step": 1140
    },
    {
      "epoch": 2.0610002242655305,
      "grad_norm": 62.858116149902344,
      "learning_rate": 6.583034647550776e-05,
      "loss": 5.143795394897461,
      "step": 1150
    },
    {
      "epoch": 2.078941466696569,
      "grad_norm": 44.81013488769531,
      "learning_rate": 6.553166069295101e-05,
      "loss": 4.362034606933594,
      "step": 1160
    },
    {
      "epoch": 2.096882709127607,
      "grad_norm": 35.57542419433594,
      "learning_rate": 6.523297491039428e-05,
      "loss": 3.999142074584961,
      "step": 1170
    },
    {
      "epoch": 2.1148239515586456,
      "grad_norm": 41.06182861328125,
      "learning_rate": 6.493428912783753e-05,
      "loss": 3.610125732421875,
      "step": 1180
    },
    {
      "epoch": 2.132765193989684,
      "grad_norm": 47.11083984375,
      "learning_rate": 6.463560334528077e-05,
      "loss": 4.454216384887696,
      "step": 1190
    },
    {
      "epoch": 2.1507064364207222,
      "grad_norm": 65.90501403808594,
      "learning_rate": 6.433691756272402e-05,
      "loss": 4.447940063476563,
      "step": 1200
    },
    {
      "epoch": 2.1686476788517606,
      "grad_norm": 46.56593322753906,
      "learning_rate": 6.403823178016727e-05,
      "loss": 3.5009899139404297,
      "step": 1210
    },
    {
      "epoch": 2.186588921282799,
      "grad_norm": 41.10649490356445,
      "learning_rate": 6.373954599761052e-05,
      "loss": 3.4816562652587892,
      "step": 1220
    },
    {
      "epoch": 2.2045301637138373,
      "grad_norm": 57.6690673828125,
      "learning_rate": 6.344086021505376e-05,
      "loss": 5.258626174926758,
      "step": 1230
    },
    {
      "epoch": 2.2224714061448756,
      "grad_norm": 73.70288848876953,
      "learning_rate": 6.314217443249701e-05,
      "loss": 5.213370513916016,
      "step": 1240
    },
    {
      "epoch": 2.240412648575914,
      "grad_norm": 60.33953857421875,
      "learning_rate": 6.284348864994026e-05,
      "loss": 4.110390472412109,
      "step": 1250
    },
    {
      "epoch": 2.2583538910069523,
      "grad_norm": 40.06295394897461,
      "learning_rate": 6.254480286738351e-05,
      "loss": 4.270783996582031,
      "step": 1260
    },
    {
      "epoch": 2.2762951334379906,
      "grad_norm": 46.57061004638672,
      "learning_rate": 6.224611708482677e-05,
      "loss": 3.8393001556396484,
      "step": 1270
    },
    {
      "epoch": 2.294236375869029,
      "grad_norm": 37.73043441772461,
      "learning_rate": 6.194743130227002e-05,
      "loss": 4.038366317749023,
      "step": 1280
    },
    {
      "epoch": 2.3121776183000673,
      "grad_norm": 30.877126693725586,
      "learning_rate": 6.164874551971327e-05,
      "loss": 4.4562946319580075,
      "step": 1290
    },
    {
      "epoch": 2.3301188607311056,
      "grad_norm": 51.45194625854492,
      "learning_rate": 6.135005973715652e-05,
      "loss": 5.070964431762695,
      "step": 1300
    },
    {
      "epoch": 2.348060103162144,
      "grad_norm": 65.69786071777344,
      "learning_rate": 6.105137395459977e-05,
      "loss": 3.7924365997314453,
      "step": 1310
    },
    {
      "epoch": 2.3660013455931823,
      "grad_norm": 51.33269500732422,
      "learning_rate": 6.0752688172043016e-05,
      "loss": 3.630891799926758,
      "step": 1320
    },
    {
      "epoch": 2.3839425880242207,
      "grad_norm": 76.02178192138672,
      "learning_rate": 6.045400238948626e-05,
      "loss": 4.390088653564453,
      "step": 1330
    },
    {
      "epoch": 2.401883830455259,
      "grad_norm": 39.55622482299805,
      "learning_rate": 6.015531660692951e-05,
      "loss": 4.653537368774414,
      "step": 1340
    },
    {
      "epoch": 2.4198250728862973,
      "grad_norm": 34.26464080810547,
      "learning_rate": 5.9856630824372764e-05,
      "loss": 3.9078521728515625,
      "step": 1350
    },
    {
      "epoch": 2.4377663153173357,
      "grad_norm": 63.53683853149414,
      "learning_rate": 5.955794504181601e-05,
      "loss": 4.869079971313477,
      "step": 1360
    },
    {
      "epoch": 2.455707557748374,
      "grad_norm": 37.18147277832031,
      "learning_rate": 5.925925925925926e-05,
      "loss": 4.531685256958008,
      "step": 1370
    },
    {
      "epoch": 2.4736488001794124,
      "grad_norm": 16.13014030456543,
      "learning_rate": 5.8960573476702505e-05,
      "loss": 3.8618179321289063,
      "step": 1380
    },
    {
      "epoch": 2.4915900426104507,
      "grad_norm": 42.828208923339844,
      "learning_rate": 5.8661887694145756e-05,
      "loss": 5.073774337768555,
      "step": 1390
    },
    {
      "epoch": 2.509531285041489,
      "grad_norm": 49.29751205444336,
      "learning_rate": 5.8363201911589014e-05,
      "loss": 4.143959045410156,
      "step": 1400
    },
    {
      "epoch": 2.5274725274725274,
      "grad_norm": 62.36171340942383,
      "learning_rate": 5.8064516129032266e-05,
      "loss": 3.9800132751464843,
      "step": 1410
    },
    {
      "epoch": 2.5454137699035657,
      "grad_norm": 52.18574142456055,
      "learning_rate": 5.776583034647551e-05,
      "loss": 3.8350910186767577,
      "step": 1420
    },
    {
      "epoch": 2.563355012334604,
      "grad_norm": 55.923851013183594,
      "learning_rate": 5.746714456391876e-05,
      "loss": 3.9334957122802736,
      "step": 1430
    },
    {
      "epoch": 2.5812962547656424,
      "grad_norm": 40.96153259277344,
      "learning_rate": 5.7168458781362014e-05,
      "loss": 3.756947708129883,
      "step": 1440
    },
    {
      "epoch": 2.5992374971966807,
      "grad_norm": 70.88421630859375,
      "learning_rate": 5.686977299880526e-05,
      "loss": 4.528082656860351,
      "step": 1450
    },
    {
      "epoch": 2.617178739627719,
      "grad_norm": 67.73676300048828,
      "learning_rate": 5.657108721624851e-05,
      "loss": 3.6640289306640623,
      "step": 1460
    },
    {
      "epoch": 2.6351199820587574,
      "grad_norm": 45.19475555419922,
      "learning_rate": 5.6272401433691755e-05,
      "loss": 4.181307220458985,
      "step": 1470
    },
    {
      "epoch": 2.6530612244897958,
      "grad_norm": 55.28110122680664,
      "learning_rate": 5.5973715651135006e-05,
      "loss": 3.82142333984375,
      "step": 1480
    },
    {
      "epoch": 2.671002466920834,
      "grad_norm": 57.95777893066406,
      "learning_rate": 5.567502986857826e-05,
      "loss": 4.602880477905273,
      "step": 1490
    },
    {
      "epoch": 2.6889437093518724,
      "grad_norm": 32.378787994384766,
      "learning_rate": 5.53763440860215e-05,
      "loss": 4.658335494995117,
      "step": 1500
    },
    {
      "epoch": 2.7068849517829108,
      "grad_norm": 49.11410140991211,
      "learning_rate": 5.507765830346476e-05,
      "loss": 3.3590938568115236,
      "step": 1510
    },
    {
      "epoch": 2.724826194213949,
      "grad_norm": 52.24827575683594,
      "learning_rate": 5.477897252090801e-05,
      "loss": 4.779837799072266,
      "step": 1520
    },
    {
      "epoch": 2.7427674366449875,
      "grad_norm": 40.396392822265625,
      "learning_rate": 5.4480286738351264e-05,
      "loss": 3.744877243041992,
      "step": 1530
    },
    {
      "epoch": 2.760708679076026,
      "grad_norm": 32.31203842163086,
      "learning_rate": 5.418160095579451e-05,
      "loss": 3.868155670166016,
      "step": 1540
    },
    {
      "epoch": 2.778649921507064,
      "grad_norm": 39.0560188293457,
      "learning_rate": 5.388291517323776e-05,
      "loss": 4.37359390258789,
      "step": 1550
    },
    {
      "epoch": 2.7965911639381025,
      "grad_norm": 59.671363830566406,
      "learning_rate": 5.3584229390681005e-05,
      "loss": 3.3381996154785156,
      "step": 1560
    },
    {
      "epoch": 2.814532406369141,
      "grad_norm": 34.25743865966797,
      "learning_rate": 5.3285543608124256e-05,
      "loss": 3.7136985778808596,
      "step": 1570
    },
    {
      "epoch": 2.832473648800179,
      "grad_norm": 81.48686981201172,
      "learning_rate": 5.29868578255675e-05,
      "loss": 4.005496978759766,
      "step": 1580
    },
    {
      "epoch": 2.8504148912312175,
      "grad_norm": 79.54581451416016,
      "learning_rate": 5.268817204301075e-05,
      "loss": 3.5920570373535154,
      "step": 1590
    },
    {
      "epoch": 2.868356133662256,
      "grad_norm": 38.96965026855469,
      "learning_rate": 5.2389486260454004e-05,
      "loss": 4.356932830810547,
      "step": 1600
    },
    {
      "epoch": 2.8862973760932946,
      "grad_norm": 56.912437438964844,
      "learning_rate": 5.209080047789725e-05,
      "loss": 3.4046165466308596,
      "step": 1610
    },
    {
      "epoch": 2.904238618524333,
      "grad_norm": 33.90089416503906,
      "learning_rate": 5.17921146953405e-05,
      "loss": 3.9069644927978517,
      "step": 1620
    },
    {
      "epoch": 2.9221798609553713,
      "grad_norm": 39.82508087158203,
      "learning_rate": 5.149342891278376e-05,
      "loss": 3.9858497619628905,
      "step": 1630
    },
    {
      "epoch": 2.9401211033864096,
      "grad_norm": 51.494178771972656,
      "learning_rate": 5.119474313022701e-05,
      "loss": 3.7873130798339845,
      "step": 1640
    },
    {
      "epoch": 2.958062345817448,
      "grad_norm": 45.71894073486328,
      "learning_rate": 5.0896057347670255e-05,
      "loss": 4.444355392456055,
      "step": 1650
    },
    {
      "epoch": 2.9760035882484863,
      "grad_norm": 49.880088806152344,
      "learning_rate": 5.0597371565113506e-05,
      "loss": 3.8044406890869142,
      "step": 1660
    },
    {
      "epoch": 2.9939448306795247,
      "grad_norm": 55.13593292236328,
      "learning_rate": 5.029868578255675e-05,
      "loss": 4.811407852172851,
      "step": 1670
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.6215358972549438,
      "eval_runtime": 170.6429,
      "eval_samples_per_second": 6.534,
      "eval_steps_per_second": 0.82,
      "step": 1674
    },
    {
      "epoch": 3.010764745458623,
      "grad_norm": 43.01554870605469,
      "learning_rate": 5e-05,
      "loss": 3.6487064361572266,
      "step": 1680
    },
    {
      "epoch": 3.028705987889661,
      "grad_norm": 37.2437858581543,
      "learning_rate": 4.9701314217443254e-05,
      "loss": 4.410398483276367,
      "step": 1690
    },
    {
      "epoch": 3.0466472303206995,
      "grad_norm": 75.95458984375,
      "learning_rate": 4.94026284348865e-05,
      "loss": 3.840514373779297,
      "step": 1700
    },
    {
      "epoch": 3.064588472751738,
      "grad_norm": 76.05812072753906,
      "learning_rate": 4.910394265232976e-05,
      "loss": 3.444409942626953,
      "step": 1710
    },
    {
      "epoch": 3.082529715182776,
      "grad_norm": 40.664512634277344,
      "learning_rate": 4.8805256869773e-05,
      "loss": 3.6067691802978517,
      "step": 1720
    },
    {
      "epoch": 3.100470957613815,
      "grad_norm": 45.09910583496094,
      "learning_rate": 4.850657108721625e-05,
      "loss": 3.0286449432373046,
      "step": 1730
    },
    {
      "epoch": 3.1184122000448533,
      "grad_norm": 54.673728942871094,
      "learning_rate": 4.82078853046595e-05,
      "loss": 3.3635635375976562,
      "step": 1740
    },
    {
      "epoch": 3.1363534424758917,
      "grad_norm": 61.46894073486328,
      "learning_rate": 4.790919952210275e-05,
      "loss": 4.302886581420898,
      "step": 1750
    },
    {
      "epoch": 3.15429468490693,
      "grad_norm": 36.994224548339844,
      "learning_rate": 4.7610513739546e-05,
      "loss": 3.622949981689453,
      "step": 1760
    },
    {
      "epoch": 3.1722359273379683,
      "grad_norm": 30.627626419067383,
      "learning_rate": 4.731182795698925e-05,
      "loss": 4.073110961914063,
      "step": 1770
    },
    {
      "epoch": 3.1901771697690067,
      "grad_norm": 46.39509963989258,
      "learning_rate": 4.7013142174432504e-05,
      "loss": 4.55328483581543,
      "step": 1780
    },
    {
      "epoch": 3.208118412200045,
      "grad_norm": 64.64254760742188,
      "learning_rate": 4.671445639187575e-05,
      "loss": 3.2528812408447267,
      "step": 1790
    },
    {
      "epoch": 3.2260596546310834,
      "grad_norm": 40.790828704833984,
      "learning_rate": 4.6415770609319e-05,
      "loss": 4.544404220581055,
      "step": 1800
    },
    {
      "epoch": 3.2440008970621217,
      "grad_norm": 62.53789138793945,
      "learning_rate": 4.6117084826762245e-05,
      "loss": 4.652935028076172,
      "step": 1810
    },
    {
      "epoch": 3.26194213949316,
      "grad_norm": 25.911144256591797,
      "learning_rate": 4.5818399044205496e-05,
      "loss": 3.436854934692383,
      "step": 1820
    },
    {
      "epoch": 3.2798833819241984,
      "grad_norm": 32.794376373291016,
      "learning_rate": 4.551971326164875e-05,
      "loss": 3.491845703125,
      "step": 1830
    },
    {
      "epoch": 3.2978246243552367,
      "grad_norm": 31.038558959960938,
      "learning_rate": 4.5221027479092e-05,
      "loss": 3.322488021850586,
      "step": 1840
    },
    {
      "epoch": 3.315765866786275,
      "grad_norm": 35.71550750732422,
      "learning_rate": 4.4922341696535244e-05,
      "loss": 3.0920780181884764,
      "step": 1850
    },
    {
      "epoch": 3.3337071092173134,
      "grad_norm": 66.03709411621094,
      "learning_rate": 4.4623655913978496e-05,
      "loss": 4.147052001953125,
      "step": 1860
    },
    {
      "epoch": 3.3516483516483517,
      "grad_norm": 52.17298126220703,
      "learning_rate": 4.432497013142175e-05,
      "loss": 3.7864940643310545,
      "step": 1870
    },
    {
      "epoch": 3.36958959407939,
      "grad_norm": 48.993038177490234,
      "learning_rate": 4.402628434886499e-05,
      "loss": 3.829874801635742,
      "step": 1880
    },
    {
      "epoch": 3.3875308365104284,
      "grad_norm": 62.20663070678711,
      "learning_rate": 4.372759856630825e-05,
      "loss": 4.848347473144531,
      "step": 1890
    },
    {
      "epoch": 3.4054720789414668,
      "grad_norm": 38.452152252197266,
      "learning_rate": 4.3428912783751495e-05,
      "loss": 2.9229183197021484,
      "step": 1900
    },
    {
      "epoch": 3.423413321372505,
      "grad_norm": 60.124549865722656,
      "learning_rate": 4.3130227001194746e-05,
      "loss": 3.7015132904052734,
      "step": 1910
    },
    {
      "epoch": 3.4413545638035434,
      "grad_norm": 48.43402862548828,
      "learning_rate": 4.283154121863799e-05,
      "loss": 3.1375972747802736,
      "step": 1920
    },
    {
      "epoch": 3.4592958062345818,
      "grad_norm": 44.126956939697266,
      "learning_rate": 4.253285543608124e-05,
      "loss": 3.321000671386719,
      "step": 1930
    },
    {
      "epoch": 3.47723704866562,
      "grad_norm": 32.88559341430664,
      "learning_rate": 4.2234169653524494e-05,
      "loss": 4.118202209472656,
      "step": 1940
    },
    {
      "epoch": 3.4951782910966585,
      "grad_norm": 50.9506950378418,
      "learning_rate": 4.1935483870967746e-05,
      "loss": 3.6180141448974608,
      "step": 1950
    },
    {
      "epoch": 3.513119533527697,
      "grad_norm": 45.271732330322266,
      "learning_rate": 4.1636798088411e-05,
      "loss": 3.8229740142822264,
      "step": 1960
    },
    {
      "epoch": 3.531060775958735,
      "grad_norm": 54.32799530029297,
      "learning_rate": 4.133811230585424e-05,
      "loss": 3.555501937866211,
      "step": 1970
    },
    {
      "epoch": 3.5490020183897735,
      "grad_norm": 46.28438186645508,
      "learning_rate": 4.1039426523297493e-05,
      "loss": 3.4468902587890624,
      "step": 1980
    },
    {
      "epoch": 3.566943260820812,
      "grad_norm": 33.24092102050781,
      "learning_rate": 4.074074074074074e-05,
      "loss": 3.369745635986328,
      "step": 1990
    },
    {
      "epoch": 3.58488450325185,
      "grad_norm": 44.27842712402344,
      "learning_rate": 4.0442054958183996e-05,
      "loss": 2.9321271896362306,
      "step": 2000
    },
    {
      "epoch": 3.6028257456828885,
      "grad_norm": 45.925987243652344,
      "learning_rate": 4.014336917562724e-05,
      "loss": 3.77596435546875,
      "step": 2010
    },
    {
      "epoch": 3.620766988113927,
      "grad_norm": 54.0433349609375,
      "learning_rate": 3.984468339307049e-05,
      "loss": 4.086590957641602,
      "step": 2020
    },
    {
      "epoch": 3.638708230544965,
      "grad_norm": 55.98088455200195,
      "learning_rate": 3.9545997610513744e-05,
      "loss": 3.2724845886230467,
      "step": 2030
    },
    {
      "epoch": 3.6566494729760035,
      "grad_norm": 26.771520614624023,
      "learning_rate": 3.924731182795699e-05,
      "loss": 3.3585472106933594,
      "step": 2040
    },
    {
      "epoch": 3.674590715407042,
      "grad_norm": 90.48389434814453,
      "learning_rate": 3.894862604540024e-05,
      "loss": 4.390112686157226,
      "step": 2050
    },
    {
      "epoch": 3.69253195783808,
      "grad_norm": 35.204097747802734,
      "learning_rate": 3.864994026284349e-05,
      "loss": 3.4097503662109374,
      "step": 2060
    },
    {
      "epoch": 3.7104732002691185,
      "grad_norm": 58.13835144042969,
      "learning_rate": 3.8351254480286743e-05,
      "loss": 3.6352352142333983,
      "step": 2070
    },
    {
      "epoch": 3.728414442700157,
      "grad_norm": 62.84121322631836,
      "learning_rate": 3.805256869772999e-05,
      "loss": 4.200895309448242,
      "step": 2080
    },
    {
      "epoch": 3.746355685131195,
      "grad_norm": 36.448814392089844,
      "learning_rate": 3.775388291517324e-05,
      "loss": 2.92446346282959,
      "step": 2090
    },
    {
      "epoch": 3.7642969275622336,
      "grad_norm": 48.65433883666992,
      "learning_rate": 3.7455197132616484e-05,
      "loss": 4.237557983398437,
      "step": 2100
    },
    {
      "epoch": 3.782238169993272,
      "grad_norm": 39.19860076904297,
      "learning_rate": 3.715651135005974e-05,
      "loss": 3.197210502624512,
      "step": 2110
    },
    {
      "epoch": 3.8001794124243102,
      "grad_norm": 56.33600997924805,
      "learning_rate": 3.685782556750299e-05,
      "loss": 4.2935138702392575,
      "step": 2120
    },
    {
      "epoch": 3.8181206548553486,
      "grad_norm": 59.2774658203125,
      "learning_rate": 3.655913978494624e-05,
      "loss": 4.592678070068359,
      "step": 2130
    },
    {
      "epoch": 3.8360618972863874,
      "grad_norm": 49.73590087890625,
      "learning_rate": 3.626045400238949e-05,
      "loss": 3.845172119140625,
      "step": 2140
    },
    {
      "epoch": 3.8540031397174257,
      "grad_norm": 49.483001708984375,
      "learning_rate": 3.5961768219832735e-05,
      "loss": 4.02205810546875,
      "step": 2150
    },
    {
      "epoch": 3.871944382148464,
      "grad_norm": 45.352569580078125,
      "learning_rate": 3.566308243727599e-05,
      "loss": 2.6222352981567383,
      "step": 2160
    },
    {
      "epoch": 3.8898856245795024,
      "grad_norm": 79.61295318603516,
      "learning_rate": 3.536439665471924e-05,
      "loss": 3.064870262145996,
      "step": 2170
    },
    {
      "epoch": 3.9078268670105407,
      "grad_norm": 42.029747009277344,
      "learning_rate": 3.506571087216249e-05,
      "loss": 3.259450912475586,
      "step": 2180
    },
    {
      "epoch": 3.925768109441579,
      "grad_norm": 65.79647064208984,
      "learning_rate": 3.4767025089605734e-05,
      "loss": 3.9480785369873046,
      "step": 2190
    },
    {
      "epoch": 3.9437093518726174,
      "grad_norm": 64.28897857666016,
      "learning_rate": 3.4468339307048986e-05,
      "loss": 3.503002166748047,
      "step": 2200
    },
    {
      "epoch": 3.9616505943036557,
      "grad_norm": 50.83932876586914,
      "learning_rate": 3.416965352449224e-05,
      "loss": 3.231386184692383,
      "step": 2210
    },
    {
      "epoch": 3.979591836734694,
      "grad_norm": 32.64017105102539,
      "learning_rate": 3.387096774193548e-05,
      "loss": 3.626300811767578,
      "step": 2220
    },
    {
      "epoch": 3.9975330791657324,
      "grad_norm": 20.845733642578125,
      "learning_rate": 3.357228195937874e-05,
      "loss": 2.5473859786987303,
      "step": 2230
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.581364095211029,
      "eval_runtime": 115.7736,
      "eval_samples_per_second": 9.631,
      "eval_steps_per_second": 1.209,
      "step": 2232
    },
    {
      "epoch": 4.014352993944831,
      "grad_norm": 58.010498046875,
      "learning_rate": 3.3273596176821985e-05,
      "loss": 2.6530553817749025,
      "step": 2240
    },
    {
      "epoch": 4.032294236375869,
      "grad_norm": 35.7950325012207,
      "learning_rate": 3.297491039426524e-05,
      "loss": 2.568526268005371,
      "step": 2250
    },
    {
      "epoch": 4.050235478806908,
      "grad_norm": 48.8873176574707,
      "learning_rate": 3.267622461170848e-05,
      "loss": 3.7057258605957033,
      "step": 2260
    },
    {
      "epoch": 4.068176721237946,
      "grad_norm": 42.1533203125,
      "learning_rate": 3.237753882915173e-05,
      "loss": 2.3085552215576173,
      "step": 2270
    },
    {
      "epoch": 4.086117963668984,
      "grad_norm": 54.59083938598633,
      "learning_rate": 3.207885304659498e-05,
      "loss": 3.5252605438232423,
      "step": 2280
    },
    {
      "epoch": 4.104059206100023,
      "grad_norm": 31.93855857849121,
      "learning_rate": 3.1780167264038236e-05,
      "loss": 3.4623226165771483,
      "step": 2290
    },
    {
      "epoch": 4.122000448531061,
      "grad_norm": 32.2172966003418,
      "learning_rate": 3.148148148148148e-05,
      "loss": 3.218683624267578,
      "step": 2300
    },
    {
      "epoch": 4.139941690962099,
      "grad_norm": 50.2918815612793,
      "learning_rate": 3.118279569892473e-05,
      "loss": 3.3993221282958985,
      "step": 2310
    },
    {
      "epoch": 4.157882933393138,
      "grad_norm": 48.154781341552734,
      "learning_rate": 3.0884109916367984e-05,
      "loss": 3.814352035522461,
      "step": 2320
    },
    {
      "epoch": 4.175824175824176,
      "grad_norm": 76.82417297363281,
      "learning_rate": 3.058542413381123e-05,
      "loss": 3.9558479309082033,
      "step": 2330
    },
    {
      "epoch": 4.193765418255214,
      "grad_norm": 39.6548957824707,
      "learning_rate": 3.0286738351254483e-05,
      "loss": 3.2936374664306642,
      "step": 2340
    },
    {
      "epoch": 4.211706660686253,
      "grad_norm": 41.142913818359375,
      "learning_rate": 2.998805256869773e-05,
      "loss": 3.524032211303711,
      "step": 2350
    },
    {
      "epoch": 4.229647903117291,
      "grad_norm": 39.23564529418945,
      "learning_rate": 2.9689366786140983e-05,
      "loss": 3.5088054656982424,
      "step": 2360
    },
    {
      "epoch": 4.2475891455483294,
      "grad_norm": 43.090614318847656,
      "learning_rate": 2.939068100358423e-05,
      "loss": 3.0264705657958983,
      "step": 2370
    },
    {
      "epoch": 4.265530387979368,
      "grad_norm": 57.40953826904297,
      "learning_rate": 2.909199522102748e-05,
      "loss": 3.6430427551269533,
      "step": 2380
    },
    {
      "epoch": 4.283471630410406,
      "grad_norm": 41.66252136230469,
      "learning_rate": 2.8793309438470727e-05,
      "loss": 2.817091941833496,
      "step": 2390
    },
    {
      "epoch": 4.3014128728414445,
      "grad_norm": 43.02223205566406,
      "learning_rate": 2.8494623655913982e-05,
      "loss": 3.5273643493652345,
      "step": 2400
    },
    {
      "epoch": 4.319354115272483,
      "grad_norm": 46.996360778808594,
      "learning_rate": 2.819593787335723e-05,
      "loss": 3.7458122253417967,
      "step": 2410
    },
    {
      "epoch": 4.337295357703521,
      "grad_norm": 57.13689422607422,
      "learning_rate": 2.789725209080048e-05,
      "loss": 2.952749252319336,
      "step": 2420
    },
    {
      "epoch": 4.3552366001345595,
      "grad_norm": 50.875999450683594,
      "learning_rate": 2.759856630824373e-05,
      "loss": 3.7385543823242187,
      "step": 2430
    },
    {
      "epoch": 4.373177842565598,
      "grad_norm": 68.8681869506836,
      "learning_rate": 2.7299880525686978e-05,
      "loss": 2.7634267807006836,
      "step": 2440
    },
    {
      "epoch": 4.391119084996636,
      "grad_norm": 47.81062316894531,
      "learning_rate": 2.7001194743130226e-05,
      "loss": 4.142981719970703,
      "step": 2450
    },
    {
      "epoch": 4.4090603274276745,
      "grad_norm": 62.63823699951172,
      "learning_rate": 2.670250896057348e-05,
      "loss": 3.275040054321289,
      "step": 2460
    },
    {
      "epoch": 4.427001569858713,
      "grad_norm": 23.603967666625977,
      "learning_rate": 2.640382317801673e-05,
      "loss": 3.103872299194336,
      "step": 2470
    },
    {
      "epoch": 4.444942812289751,
      "grad_norm": 56.62980651855469,
      "learning_rate": 2.6105137395459977e-05,
      "loss": 3.771506118774414,
      "step": 2480
    },
    {
      "epoch": 4.4628840547207895,
      "grad_norm": 49.8080940246582,
      "learning_rate": 2.5806451612903226e-05,
      "loss": 3.095960998535156,
      "step": 2490
    },
    {
      "epoch": 4.480825297151828,
      "grad_norm": 63.05908966064453,
      "learning_rate": 2.5507765830346474e-05,
      "loss": 3.448394012451172,
      "step": 2500
    },
    {
      "epoch": 4.498766539582866,
      "grad_norm": 45.5593376159668,
      "learning_rate": 2.5209080047789725e-05,
      "loss": 3.193262481689453,
      "step": 2510
    },
    {
      "epoch": 4.5167077820139045,
      "grad_norm": 38.76456069946289,
      "learning_rate": 2.4910394265232977e-05,
      "loss": 3.785431671142578,
      "step": 2520
    },
    {
      "epoch": 4.534649024444943,
      "grad_norm": 49.53538131713867,
      "learning_rate": 2.4611708482676228e-05,
      "loss": 2.5413747787475587,
      "step": 2530
    },
    {
      "epoch": 4.552590266875981,
      "grad_norm": 43.09889602661133,
      "learning_rate": 2.4313022700119476e-05,
      "loss": 2.9614273071289063,
      "step": 2540
    },
    {
      "epoch": 4.57053150930702,
      "grad_norm": 39.87770462036133,
      "learning_rate": 2.4014336917562724e-05,
      "loss": 2.9629632949829103,
      "step": 2550
    },
    {
      "epoch": 4.588472751738058,
      "grad_norm": 68.67454528808594,
      "learning_rate": 2.3715651135005976e-05,
      "loss": 2.880708122253418,
      "step": 2560
    },
    {
      "epoch": 4.606413994169096,
      "grad_norm": 76.92101287841797,
      "learning_rate": 2.3416965352449224e-05,
      "loss": 4.098785781860352,
      "step": 2570
    },
    {
      "epoch": 4.624355236600135,
      "grad_norm": 45.37848663330078,
      "learning_rate": 2.3118279569892472e-05,
      "loss": 3.425364685058594,
      "step": 2580
    },
    {
      "epoch": 4.642296479031173,
      "grad_norm": 27.85301399230957,
      "learning_rate": 2.2819593787335724e-05,
      "loss": 3.4158321380615235,
      "step": 2590
    },
    {
      "epoch": 4.660237721462211,
      "grad_norm": 48.370361328125,
      "learning_rate": 2.2520908004778972e-05,
      "loss": 2.9740795135498046,
      "step": 2600
    },
    {
      "epoch": 4.67817896389325,
      "grad_norm": 69.7438735961914,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 3.3067596435546873,
      "step": 2610
    },
    {
      "epoch": 4.696120206324288,
      "grad_norm": 55.6045036315918,
      "learning_rate": 2.1923536439665475e-05,
      "loss": 3.0386404037475585,
      "step": 2620
    },
    {
      "epoch": 4.714061448755326,
      "grad_norm": 38.612770080566406,
      "learning_rate": 2.1624850657108723e-05,
      "loss": 3.103988265991211,
      "step": 2630
    },
    {
      "epoch": 4.732002691186365,
      "grad_norm": 44.75177764892578,
      "learning_rate": 2.132616487455197e-05,
      "loss": 3.519016647338867,
      "step": 2640
    },
    {
      "epoch": 4.749943933617403,
      "grad_norm": 29.089937210083008,
      "learning_rate": 2.1027479091995223e-05,
      "loss": 3.474700164794922,
      "step": 2650
    },
    {
      "epoch": 4.767885176048441,
      "grad_norm": 51.11640930175781,
      "learning_rate": 2.072879330943847e-05,
      "loss": 4.124442291259766,
      "step": 2660
    },
    {
      "epoch": 4.78582641847948,
      "grad_norm": 57.63646697998047,
      "learning_rate": 2.0430107526881722e-05,
      "loss": 3.2044700622558593,
      "step": 2670
    },
    {
      "epoch": 4.803767660910518,
      "grad_norm": 66.78971862792969,
      "learning_rate": 2.013142174432497e-05,
      "loss": 3.828175354003906,
      "step": 2680
    },
    {
      "epoch": 4.821708903341556,
      "grad_norm": 25.655420303344727,
      "learning_rate": 1.9832735961768222e-05,
      "loss": 3.7521087646484377,
      "step": 2690
    },
    {
      "epoch": 4.839650145772595,
      "grad_norm": 50.73670959472656,
      "learning_rate": 1.9534050179211473e-05,
      "loss": 3.3054824829101563,
      "step": 2700
    },
    {
      "epoch": 4.857591388203633,
      "grad_norm": 35.842124938964844,
      "learning_rate": 1.923536439665472e-05,
      "loss": 3.6853145599365233,
      "step": 2710
    },
    {
      "epoch": 4.875532630634671,
      "grad_norm": 51.76504135131836,
      "learning_rate": 1.893667861409797e-05,
      "loss": 3.2742435455322267,
      "step": 2720
    },
    {
      "epoch": 4.89347387306571,
      "grad_norm": 34.737735748291016,
      "learning_rate": 1.863799283154122e-05,
      "loss": 3.134039306640625,
      "step": 2730
    },
    {
      "epoch": 4.911415115496748,
      "grad_norm": 62.29776382446289,
      "learning_rate": 1.833930704898447e-05,
      "loss": 3.050428009033203,
      "step": 2740
    },
    {
      "epoch": 4.929356357927786,
      "grad_norm": 52.53035354614258,
      "learning_rate": 1.8040621266427717e-05,
      "loss": 3.0646501541137696,
      "step": 2750
    },
    {
      "epoch": 4.947297600358825,
      "grad_norm": 28.668615341186523,
      "learning_rate": 1.774193548387097e-05,
      "loss": 3.0033369064331055,
      "step": 2760
    },
    {
      "epoch": 4.965238842789863,
      "grad_norm": 43.82712173461914,
      "learning_rate": 1.7443249701314217e-05,
      "loss": 3.0149654388427733,
      "step": 2770
    },
    {
      "epoch": 4.983180085220901,
      "grad_norm": 30.35148811340332,
      "learning_rate": 1.714456391875747e-05,
      "loss": 3.7469856262207033,
      "step": 2780
    },
    {
      "epoch": 5.0,
      "grad_norm": 25.069307327270508,
      "learning_rate": 1.684587813620072e-05,
      "loss": 3.582502746582031,
      "step": 2790
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.5646156072616577,
      "eval_runtime": 228.3261,
      "eval_samples_per_second": 4.883,
      "eval_steps_per_second": 0.613,
      "step": 2790
    },
    {
      "epoch": 5.017941242431038,
      "grad_norm": 49.76359558105469,
      "learning_rate": 1.6547192353643968e-05,
      "loss": 3.038595962524414,
      "step": 2800
    },
    {
      "epoch": 5.035882484862077,
      "grad_norm": 52.43717575073242,
      "learning_rate": 1.6248506571087216e-05,
      "loss": 3.7384395599365234,
      "step": 2810
    },
    {
      "epoch": 5.053823727293115,
      "grad_norm": 56.98786163330078,
      "learning_rate": 1.5949820788530468e-05,
      "loss": 2.994548225402832,
      "step": 2820
    },
    {
      "epoch": 5.071764969724153,
      "grad_norm": 56.279762268066406,
      "learning_rate": 1.5651135005973716e-05,
      "loss": 2.4636568069458007,
      "step": 2830
    },
    {
      "epoch": 5.089706212155192,
      "grad_norm": 55.50758361816406,
      "learning_rate": 1.5352449223416964e-05,
      "loss": 3.1582515716552733,
      "step": 2840
    },
    {
      "epoch": 5.10764745458623,
      "grad_norm": 59.040897369384766,
      "learning_rate": 1.5053763440860215e-05,
      "loss": 3.178448295593262,
      "step": 2850
    },
    {
      "epoch": 5.125588697017268,
      "grad_norm": 33.89802551269531,
      "learning_rate": 1.4755077658303465e-05,
      "loss": 3.19612979888916,
      "step": 2860
    },
    {
      "epoch": 5.143529939448307,
      "grad_norm": 49.851226806640625,
      "learning_rate": 1.4456391875746717e-05,
      "loss": 2.9137510299682616,
      "step": 2870
    },
    {
      "epoch": 5.161471181879345,
      "grad_norm": 43.54951477050781,
      "learning_rate": 1.4157706093189965e-05,
      "loss": 2.9047920227050783,
      "step": 2880
    },
    {
      "epoch": 5.179412424310383,
      "grad_norm": 35.93354415893555,
      "learning_rate": 1.3859020310633215e-05,
      "loss": 4.175958633422852,
      "step": 2890
    },
    {
      "epoch": 5.197353666741422,
      "grad_norm": 36.019805908203125,
      "learning_rate": 1.3560334528076466e-05,
      "loss": 3.2048133850097655,
      "step": 2900
    },
    {
      "epoch": 5.21529490917246,
      "grad_norm": 45.75638961791992,
      "learning_rate": 1.3261648745519714e-05,
      "loss": 2.751006317138672,
      "step": 2910
    },
    {
      "epoch": 5.233236151603498,
      "grad_norm": 56.071311950683594,
      "learning_rate": 1.2962962962962962e-05,
      "loss": 3.7571155548095705,
      "step": 2920
    },
    {
      "epoch": 5.251177394034537,
      "grad_norm": 54.42771911621094,
      "learning_rate": 1.2664277180406214e-05,
      "loss": 2.869022560119629,
      "step": 2930
    },
    {
      "epoch": 5.269118636465575,
      "grad_norm": 79.48004913330078,
      "learning_rate": 1.2365591397849464e-05,
      "loss": 3.425436782836914,
      "step": 2940
    },
    {
      "epoch": 5.287059878896613,
      "grad_norm": 54.158199310302734,
      "learning_rate": 1.2066905615292714e-05,
      "loss": 2.686130905151367,
      "step": 2950
    },
    {
      "epoch": 5.305001121327652,
      "grad_norm": 59.33859634399414,
      "learning_rate": 1.1768219832735962e-05,
      "loss": 2.932290267944336,
      "step": 2960
    },
    {
      "epoch": 5.32294236375869,
      "grad_norm": 49.95718765258789,
      "learning_rate": 1.1469534050179212e-05,
      "loss": 2.8040937423706054,
      "step": 2970
    },
    {
      "epoch": 5.3408836061897285,
      "grad_norm": 33.26406478881836,
      "learning_rate": 1.1170848267622461e-05,
      "loss": 2.500326919555664,
      "step": 2980
    },
    {
      "epoch": 5.358824848620767,
      "grad_norm": 52.30258560180664,
      "learning_rate": 1.0872162485065711e-05,
      "loss": 2.6979494094848633,
      "step": 2990
    },
    {
      "epoch": 5.376766091051805,
      "grad_norm": 57.751251220703125,
      "learning_rate": 1.0573476702508961e-05,
      "loss": 2.576171875,
      "step": 3000
    },
    {
      "epoch": 5.3947073334828435,
      "grad_norm": 43.874752044677734,
      "learning_rate": 1.027479091995221e-05,
      "loss": 4.530358505249024,
      "step": 3010
    },
    {
      "epoch": 5.412648575913882,
      "grad_norm": 33.188419342041016,
      "learning_rate": 9.97610513739546e-06,
      "loss": 3.991609573364258,
      "step": 3020
    },
    {
      "epoch": 5.43058981834492,
      "grad_norm": 37.70429611206055,
      "learning_rate": 9.67741935483871e-06,
      "loss": 3.1251043319702148,
      "step": 3030
    },
    {
      "epoch": 5.4485310607759585,
      "grad_norm": 28.95403289794922,
      "learning_rate": 9.37873357228196e-06,
      "loss": 2.726171875,
      "step": 3040
    },
    {
      "epoch": 5.466472303206997,
      "grad_norm": 36.616111755371094,
      "learning_rate": 9.08004778972521e-06,
      "loss": 3.0357440948486327,
      "step": 3050
    },
    {
      "epoch": 5.484413545638035,
      "grad_norm": 37.44941711425781,
      "learning_rate": 8.781362007168458e-06,
      "loss": 3.6989826202392577,
      "step": 3060
    },
    {
      "epoch": 5.5023547880690735,
      "grad_norm": 80.09501647949219,
      "learning_rate": 8.48267622461171e-06,
      "loss": 4.129254531860352,
      "step": 3070
    },
    {
      "epoch": 5.520296030500112,
      "grad_norm": 73.08029174804688,
      "learning_rate": 8.18399044205496e-06,
      "loss": 2.2382469177246094,
      "step": 3080
    },
    {
      "epoch": 5.53823727293115,
      "grad_norm": 64.67284393310547,
      "learning_rate": 7.885304659498208e-06,
      "loss": 3.6394699096679686,
      "step": 3090
    },
    {
      "epoch": 5.5561785153621885,
      "grad_norm": 83.92782592773438,
      "learning_rate": 7.586618876941457e-06,
      "loss": 3.4671173095703125,
      "step": 3100
    },
    {
      "epoch": 5.574119757793227,
      "grad_norm": 41.92440414428711,
      "learning_rate": 7.287933094384708e-06,
      "loss": 3.761492919921875,
      "step": 3110
    },
    {
      "epoch": 5.592061000224265,
      "grad_norm": 39.97547149658203,
      "learning_rate": 6.989247311827957e-06,
      "loss": 3.1393106460571287,
      "step": 3120
    },
    {
      "epoch": 5.610002242655304,
      "grad_norm": 28.519847869873047,
      "learning_rate": 6.690561529271207e-06,
      "loss": 3.6141910552978516,
      "step": 3130
    },
    {
      "epoch": 5.627943485086342,
      "grad_norm": 53.01471710205078,
      "learning_rate": 6.391875746714457e-06,
      "loss": 3.048394775390625,
      "step": 3140
    },
    {
      "epoch": 5.64588472751738,
      "grad_norm": 44.53531265258789,
      "learning_rate": 6.0931899641577065e-06,
      "loss": 3.6448169708251954,
      "step": 3150
    },
    {
      "epoch": 5.663825969948419,
      "grad_norm": 38.709495544433594,
      "learning_rate": 5.794504181600956e-06,
      "loss": 2.6525089263916017,
      "step": 3160
    },
    {
      "epoch": 5.681767212379457,
      "grad_norm": 33.850181579589844,
      "learning_rate": 5.495818399044206e-06,
      "loss": 2.756997489929199,
      "step": 3170
    },
    {
      "epoch": 5.699708454810495,
      "grad_norm": 22.88532829284668,
      "learning_rate": 5.197132616487455e-06,
      "loss": 2.196630668640137,
      "step": 3180
    },
    {
      "epoch": 5.717649697241534,
      "grad_norm": 19.599550247192383,
      "learning_rate": 4.898446833930706e-06,
      "loss": 2.977467918395996,
      "step": 3190
    },
    {
      "epoch": 5.735590939672572,
      "grad_norm": 53.317108154296875,
      "learning_rate": 4.599761051373955e-06,
      "loss": 3.4843650817871095,
      "step": 3200
    },
    {
      "epoch": 5.75353218210361,
      "grad_norm": 70.33790588378906,
      "learning_rate": 4.3010752688172045e-06,
      "loss": 3.201206588745117,
      "step": 3210
    },
    {
      "epoch": 5.7714734245346495,
      "grad_norm": 40.2017936706543,
      "learning_rate": 4.002389486260454e-06,
      "loss": 2.8177669525146483,
      "step": 3220
    },
    {
      "epoch": 5.789414666965687,
      "grad_norm": 51.483795166015625,
      "learning_rate": 3.7037037037037037e-06,
      "loss": 3.302494430541992,
      "step": 3230
    },
    {
      "epoch": 5.807355909396726,
      "grad_norm": 27.354843139648438,
      "learning_rate": 3.405017921146954e-06,
      "loss": 2.968421745300293,
      "step": 3240
    },
    {
      "epoch": 5.825297151827764,
      "grad_norm": 74.57039642333984,
      "learning_rate": 3.1063321385902034e-06,
      "loss": 3.246085357666016,
      "step": 3250
    },
    {
      "epoch": 5.843238394258803,
      "grad_norm": 84.33830261230469,
      "learning_rate": 2.8076463560334528e-06,
      "loss": 3.4759620666503905,
      "step": 3260
    },
    {
      "epoch": 5.86117963668984,
      "grad_norm": 65.36066436767578,
      "learning_rate": 2.5089605734767026e-06,
      "loss": 2.8322677612304688,
      "step": 3270
    },
    {
      "epoch": 5.8791208791208796,
      "grad_norm": 87.48539733886719,
      "learning_rate": 2.2102747909199524e-06,
      "loss": 3.344232940673828,
      "step": 3280
    },
    {
      "epoch": 5.897062121551917,
      "grad_norm": 34.85520553588867,
      "learning_rate": 1.9115890083632018e-06,
      "loss": 2.3796152114868163,
      "step": 3290
    },
    {
      "epoch": 5.915003363982956,
      "grad_norm": 67.71920776367188,
      "learning_rate": 1.6129032258064516e-06,
      "loss": 3.8938278198242187,
      "step": 3300
    },
    {
      "epoch": 5.932944606413994,
      "grad_norm": 33.035648345947266,
      "learning_rate": 1.3142174432497014e-06,
      "loss": 2.3956165313720703,
      "step": 3310
    },
    {
      "epoch": 5.950885848845033,
      "grad_norm": 64.14936828613281,
      "learning_rate": 1.015531660692951e-06,
      "loss": 2.532230567932129,
      "step": 3320
    },
    {
      "epoch": 5.968827091276071,
      "grad_norm": 53.95146560668945,
      "learning_rate": 7.168458781362007e-07,
      "loss": 3.2325981140136717,
      "step": 3330
    },
    {
      "epoch": 5.98676833370711,
      "grad_norm": 59.12485885620117,
      "learning_rate": 4.1816009557945043e-07,
      "loss": 2.670486831665039,
      "step": 3340
    }
  ],
  "logging_steps": 10,
  "max_steps": 3348,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.74662604619776e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
